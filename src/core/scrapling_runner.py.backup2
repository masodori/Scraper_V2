#!/usr/bin/env python3
"""
Scrapling Integration Module

This module handles the automated scraping execution using templates
generated from interactive sessions. It integrates with Scrapling's
powerful scraping capabilities while maintaining separation from
the interactive layer.
"""

import logging
import time
import json
import csv
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import sys
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

from scrapling.fetchers import PlayWrightFetcher, StealthyFetcher
from scrapling import Adaptor

from ..models.scraping_template import ScrapingTemplate, ScrapingResult, ElementSelector, NavigationAction, CookieData

logger = logging.getLogger(__name__)


class ProgressTracker:
    """
    Terminal progress bar for scraping operations.
    """
    
    def __init__(self, total: int, description: str = "Progress"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = time.time()
        
    def update(self, increment: int = 1, description: str = None):
        """Update progress bar."""
        self.current += increment
        if description:
            self.description = description
        self._display_progress()
    
    def _display_progress(self):
        """Display the progress bar."""
        if self.total == 0:
            return
            
        percent = (self.current / self.total) * 100
        bar_length = 40
        filled_length = int(bar_length * self.current // self.total)
        
        bar = '█' * filled_length + '░' * (bar_length - filled_length)
        elapsed = time.time() - self.start_time
        
        if self.current > 0:
            eta = (elapsed / self.current) * (self.total - self.current)
            eta_str = f" ETA: {int(eta)}s" if eta > 1 else ""
        else:
            eta_str = ""
        
        # Clear line and print progress
        sys.stdout.write(f'\r{self.description}: |{bar}| {self.current}/{self.total} ({percent:.1f}%){eta_str}')
        sys.stdout.flush()
        
        if self.current >= self.total:
            print()  # New line when complete
    
    def finish(self, description: str = "Complete"):
        """Mark progress as finished."""
        self.current = self.total
        self.description = description
        self._display_progress()


class ScrapingContext:
    """
    Shared context for all scraping components.
    Holds common state and provides unified access to resources.
    """
    
    def __init__(self, template: ScrapingTemplate):
        self.template = template
        self.fetcher = None
        self.current_page = None
        self.session_logger = logging.getLogger(f"scrapling_runner.{template.name}")


class TemplateAnalyzer:
    """
    Analyzes template characteristics and determines scraping strategies.
    """
    
    def __init__(self, context: ScrapingContext):
        self.context = context
    
    def looks_like_directory_template(self) -> bool:
        """
        Determine if template appears to be designed for directory/listing pages.
        Enhanced detection for lawyer directory patterns.
        
        Returns:
            True if template has directory-like characteristics
        """
        directory_indicators = [
            # Container-based extraction suggests multiple items
            any(hasattr(elem, 'is_container') and elem.is_container for elem in self.context.template.elements),
            # Multiple elements suggests listing
            any(elem.is_multiple for elem in self.context.template.elements),
            # Directory-like selectors
            any('people' in elem.selector.lower() or 'list' in elem.selector.lower() 
                or 'grid' in elem.selector.lower() for elem in self.context.template.elements),
            # Profile link extraction suggests directory
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and 
                any('link' in sub.get('label', '').lower() or 'profile' in sub.get('label', '').lower() 
                    or 'email' in sub.get('label', '').lower()
                    for sub in elem.sub_elements if isinstance(sub, dict))
                for elem in self.context.template.elements if hasattr(elem, 'sub_elements')),
            # Actions that navigate to profiles
            any('link' in action.label.lower() and '/lawyer/' in getattr(action, 'target_url', '')
                for action in self.context.template.actions if hasattr(self.context.template, 'actions')),
            # Profile-like sub-element patterns (name, title, email combinations suggest directory)
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and
                len([sub for sub in elem.sub_elements if isinstance(sub, dict) and 
                     any(keyword in sub.get('label', '').lower() 
                         for keyword in ['name', 'title', 'position', 'email', 'phone'])]) >= 2
                for elem in self.context.template.elements if hasattr(elem, 'sub_elements'))
        ]
        
        return any(directory_indicators)
    
    def template_needs_subpage_data(self) -> bool:
        """
        Check if template has elements that require subpage navigation.
        
        Returns:
            True if template has subpage elements or education/credentials elements
        """
        # Check for elements that are typically found on individual profile pages
        subpage_indicators = [
            # Elements with subpage_elements defined
            any(hasattr(elem, 'subpage_elements') and elem.subpage_elements 
                for elem in self.context.template.elements),
            # Education/credentials elements (typically on individual pages)
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and
                any('education' in sub.get('label', '').lower() or 'cred' in sub.get('label', '').lower()
                    for sub in elem.sub_elements if isinstance(sub, dict))
                for elem in self.context.template.elements if hasattr(elem, 'sub_elements')),
            # Multiple containers with different purposes (main + subpage data)
            len([elem for elem in self.context.template.elements if hasattr(elem, 'is_container') and elem.is_container]) >= 2,
            # Actions that navigate to specific pages
            any('link' in action.label.lower() and '/lawyer/' in getattr(action, 'target_url', '')
                for action in self.context.template.actions if hasattr(self.context.template, 'actions'))
        ]
        
        return any(subpage_indicators)
    
    def is_subpage_container(self, element_config: ElementSelector) -> bool:
        """
        Check if a container is meant to extract data from individual subpages.
        
        Args:
            element_config: ElementSelector configuration
            
        Returns:
            True if container should extract data from subpages
        """
        subpage_indicators = [
            # Container label suggests subpage data
            any(keyword in element_config.label.lower() 
                for keyword in ['subpage', 'sublink', 'subcon', 'education', 'credential', 'experience', 'bio', 'profile']),
            # Sub-elements that are typically on individual pages
            hasattr(element_config, 'sub_elements') and element_config.sub_elements and
            any(keyword in sub.get('label', '').lower() if isinstance(sub, dict) else getattr(sub, 'label', '').lower()
                for keyword in ['education', 'credential', 'admission', 'bar', 'experience', 'bio', 'creds']
                for sub in element_config.sub_elements),
            # Template has follow_links enabled for this container
            getattr(element_config, 'follow_links', False)
        ]
        
        is_subpage = any(subpage_indicators)
        self.context.session_logger.debug(f"Subpage container check for '{element_config.label}': {is_subpage} (indicators: {subpage_indicators})")
        return is_subpage


class SelectorEngine:
    """
    Handles selector enhancement, fallback generation, and CSS/XPath conversion.
    """
    
    def __init__(self, context: ScrapingContext):
        self.context = context
    
    def map_generic_selector(self, sub_element: dict, context: str = "directory") -> str:
        """
        Map generic selectors to meaningful ones based on label and context.
        
        Args:
            sub_element: Sub-element configuration dict
            context: Context type ("directory" or "profile")
            
        Returns:
            Enhanced selector string
        """
        label = sub_element.get('label', '').lower()
        original_selector = sub_element.get('selector', '')
        
        # Always preserve XPath selectors - they are position-specific and already optimized
        if original_selector.startswith('xpath:'):
            return original_selector
        
        # If selector is already specific, keep it
        if len(original_selector) > 10 and ('.' in original_selector or '#' in original_selector or '[' in original_selector):
            return original_selector
        
        # Map generic selectors based on label
        if 'name' in label:
            if context == "directory":
                return "strong, p.name strong, h3, h2, .name, [class*='name'] strong, strong:first-of-type"
            else:
                return "h1, h2, .entry-title, .page-title, .lawyer-name, .attorney-name"
                
        elif any(keyword in label for keyword in ['title', 'position', 'job']):
            if context == "directory":
                return ".title, .position, .job-title, [class*='title'], [class*='position'], p.title span:first-child, span[class*='position']"
            else:
                return ".position, .title, .job-title, h2 + p, h1 + p"
                
        elif any(keyword in label for keyword in ['email', 'mail']):
            return "a[href^='mailto:'], p.contact-details a[href^='mailto:'], .email, [class*='email'], a[href*='@']"
            
        elif 'phone' in label:
            return "a[href^='tel:'], .phone, [class*='phone'], a[href*='tel']"
            
        elif any(keyword in label for keyword in ['sector', 'practice', 'area']):
            if context == "directory":
                return ".practice-area, .sector, [class*='practice'], [class*='sector'], p.contact-details span:not([class*='position']), p.title span:last-child, span[class*='practice']"
            else:
                return ".practice-area, .capabilities, .focus-areas, a[href*='/practice/']"
                
        elif any(keyword in label for keyword in ['link', 'profile', 'url']) or 'email' in label:
            if 'email' in label:
                return "a[href^='mailto:'], .email, [class*='email'], a[href*='@']"
            else:
                return "a[href*='/lawyer/'], a[href*='/attorney/'], a[href*='/people/'], a[href*='/team/'], a"
            
        elif 'education' in label:
            # More specific selectors for education to avoid duplication
            if 'education2' in label:
                return ".education li:nth-of-type(n+2), ul[class*='education'] li:nth-of-type(n+2)"
            else:
                return ".education li:first-child, .education li:first-of-type, ul[class*='education'] li:first-child"
            
        elif any(keyword in label for keyword in ['cred', 'admission', 'bar']):
            # More specific selectors for credentials to avoid duplication
            if 'creds2' in label:
                return ".admissions li:nth-of-type(n+2), .bar li:nth-of-type(n+2), .credentials li:nth-of-type(n+2)"
            else:
                return ".admissions li:first-child, .bar li:first-child, .credentials li:first-child"
        
        # Default fallback - return original
        return original_selector


class ScraplingRunner:
    """
    Executes automated scraping using Scrapling based on interactive templates.
    
    This class provides a clean separation between the interactive template
    creation and the automated scraping execution, allowing templates to be
    run in headless server environments.
    """
    
    def __init__(self, template: ScrapingTemplate):
        """
        Initialize the Scrapling runner with a template.
        
        Args:
            template: The scraping template to execute
        """
        self.template = template
        self.fetcher = None
        self.fetcher_instance = None  # Single browser instance
        self.current_page = None
        self.browser_pages = []  # Track all open pages for cleanup
        
        # Setup logging for this session
        self.session_logger = logging.getLogger(f"scrapling_runner.{template.name}")
        
        # Suppress known warnings from third-party libraries
        self._configure_logging()
    
    def _configure_logging(self) -> None:
        """Configure logging to suppress known warnings from third-party libraries."""
        import warnings
        
        # Suppress specific deprecated API warnings
        warnings.filterwarnings("ignore", message=".*deprecated.*", category=DeprecationWarning)
        warnings.filterwarnings("ignore", message=".*Scrapling.*deprecated.*", category=UserWarning)
        
        # Configure logging levels for third-party libraries
        logging.getLogger('scrapling').setLevel(logging.ERROR)
        logging.getLogger('playwright').setLevel(logging.ERROR)
        logging.getLogger('urllib3').setLevel(logging.WARNING)
        
    def execute_scraping(self) -> ScrapingResult:
        """
        Execute the complete scraping process based on the template.
        
        Returns:
            ScrapingResult containing the scraped data and metadata
        """
        result = ScrapingResult(
            template_name=self.template.name,
            url=self.template.url,
            success=False,
            data={},
            errors=[],
            metadata={}
        )
        
        try:
            logger.info(f"Starting scraping session for: {self.template.name}")
            logger.info(f"Target URL: {self.template.url}")
            
            # Initialize the fetcher
            self._initialize_fetcher()
            
            # Smart URL detection - if template looks like directory scraping but URL is individual page, try directory
            target_url = self.template.url
            if self._looks_like_directory_template() and '/lawyer/' in target_url:
                # Try to detect if this should be a directory page instead
                if 'gibsondunn.com' in target_url:
                    directory_url = target_url.split('/lawyer/')[0] + '/people/'
                    logger.info(f"Template appears to be for directory scraping, trying directory URL: {directory_url}")
                    directory_page = self._fetch_page(directory_url)
                    if directory_page:
                        target_url = directory_url
                        logger.info(f"Successfully switched to directory URL: {directory_url}")
            
            # Fetch the initial page
            self.current_page = self._fetch_page(target_url)
            if not self.current_page:
                raise Exception("Failed to fetch the initial page")
            
            # Auto-detect and handle infinite scroll for directory pages
            if self._looks_like_directory_template():
                logger.info("Directory template detected - checking for infinite scroll content")
                self._auto_scroll_to_load_all_content()
            
            # Handle pagination FIRST to collect all containers across pages
            # For directory templates, always attempt pagination even if not configured
            if (hasattr(self.template, 'pagination') and self.template.pagination) or self._looks_like_directory_template():
                logger.info("Pagination detected or directory template - processing all pages/content")
                scraped_data = self._handle_filters_and_pagination()
            else:
                # Single page extraction only
                logger.info("No pagination - extracting current page only")
                scraped_data = self._extract_data()
            
            # Execute any actions if specified (ONLY for non-navigation actions)
            if self.template.actions:
                # Check if this is a directory template with container workflow
                is_directory_workflow = self._looks_like_directory_template()
                
                # Filter out navigation actions that would interfere with pagination
                non_navigation_actions = []
                for action in self.template.actions:
                    # Skip actions that navigate to individual pages
                    if not (action.action_type == 'click' and any(
                        keyword in action.selector.lower() 
                        for keyword in ['strong', 'name', 'link', 'profile', 'lawyer']
                    )):
                        # For directory templates, also skip generic selectors that cause conflicts
                        if is_directory_workflow and action.selector in ['a', 'button', 'span', 'div']:
                            logger.info(f"Skipping generic action selector '{action.selector}' in directory workflow")
                            continue
                        non_navigation_actions.append(action)
                
                if non_navigation_actions:
                    logger.info(f"Executing {len(non_navigation_actions)} non-navigation actions")
                    original_actions = self.template.actions
                    self.template.actions = non_navigation_actions
                    action_results = self._execute_actions()
                    self.template.actions = original_actions
                    scraped_data['actions_executed'] = action_results
                else:
                    logger.info("Skipping all actions - navigation actions or generic selectors detected")
            
            # Process subpage data if any elements have follow_links enabled
            scraped_data = self._process_subpage_extractions(scraped_data)
            
            # Process containers with follow_links for subpage navigation
            scraped_data = self._process_container_subpages(scraped_data)
            
            # Merge directory and subpage data into unified format
            scraped_data = self._merge_directory_with_subpage_data(scraped_data)
            
            result.data = scraped_data
            result.success = True
            result.metadata = {
                'elements_found': len([k for k in scraped_data.keys() if k != 'actions_executed']),
                'actions_executed': len(self.template.actions),
                'page_title': self._get_page_title(),
                'final_url': self.current_page.url if self.current_page else self.template.url
            }
            
            logger.info(f"Scraping completed successfully. Found {result.metadata['elements_found']} elements.")
            
        except Exception as e:
            error_msg = f"Scraping failed: {str(e)}"
            logger.error(error_msg)
            result.errors.append(error_msg)
            result.success = False
            
        finally:
            self._cleanup()
        
        return result
    
    def _looks_like_directory_template(self) -> bool:
        """
        Determine if template appears to be designed for directory/listing pages.
        Enhanced detection for directory patterns.
        
        Returns:
            True if template has directory-like characteristics
        """
        directory_indicators = [
            # Container-based extraction suggests multiple items
            any(hasattr(elem, 'is_container') and elem.is_container for elem in self.template.elements),
            # Multiple elements suggests listing
            any(elem.is_multiple for elem in self.template.elements),
            # Directory-like selectors
            any('people' in elem.selector.lower() or 'list' in elem.selector.lower() 
                or 'grid' in elem.selector.lower() for elem in self.template.elements),
            # Profile link extraction suggests directory
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and 
                any('link' in sub.get('label', '').lower() or 'profile' in sub.get('label', '').lower() 
                    or 'email' in sub.get('label', '').lower()
                    for sub in elem.sub_elements if isinstance(sub, dict))
                for elem in self.template.elements if hasattr(elem, 'sub_elements')),
            # Actions that navigate to profiles
            any('link' in action.label.lower() and '/lawyer/' in getattr(action, 'target_url', '')
                for action in self.template.actions if hasattr(self.template, 'actions')),
            # Profile-like sub-element patterns (name, title, email combinations suggest directory)
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and
                len([sub for sub in elem.sub_elements if isinstance(sub, dict) and 
                     any(keyword in sub.get('label', '').lower() 
                         for keyword in ['name', 'title', 'position', 'email', 'phone'])]) >= 2
                for elem in self.template.elements if hasattr(elem, 'sub_elements'))
        ]
        
        return any(directory_indicators)
    
    def _find_directory_containers(self, element_config: ElementSelector) -> List:
        """
        Smart detection of lawyer profile containers on directory pages.
        
        Args:
            element_config: ElementSelector configuration
            
        Returns:
            List of container elements representing lawyer profiles
        """
        # Try Gibson Dunn specific patterns first - these are the actual containers on the site
        directory_patterns = [
            # Gibson Dunn specific - look for elements with profile links
            "*[data-wpgb-filter-item] .wp-block-column",  # Actual lawyer cards with filter data
            "div.wp-block-column:has(a[href*='/lawyer/'])",  # Columns containing lawyer links
            ".wp-grid-builder .wp-block-column",  # Grid builder columns
            "div.wp-block-column",  # Each lawyer card container
            "[class*='wp-block-column']",
            # Legacy patterns
            ".people.loading",  
            ".people-card", 
            ".lawyer-card",
            ".attorney-card",
            ".profile-card",
            "[class*='people']",
            "[class*='lawyer']",
            "[class*='attorney']",
            "[class*='profile']",
            ".directory-item",
            ".staff-member"
        ]
        
        for pattern in directory_patterns:
            try:
                elements = self.current_page.css(pattern)
                if elements and len(elements) >= 3:  # Need at least 3 for a directory
                    logger.info(f"Found {len(elements)} directory containers with pattern: {pattern}")
                    return elements
            except Exception:
                continue
        
        # Try content-based detection for lawyer profiles specifically
        content_patterns = [
            # Look for elements containing profile links
            "//*[.//a[contains(@href, '/lawyer/')]]",
            # Look for elements with lawyer-like content
            "//*[contains(@class, 'column') and .//a[contains(@href, '/lawyer/')]]",
            # Look for cards with partner/associate content
            "//*[contains(@class, 'card') and contains(., 'Partner')]",
            "//*[contains(@class, 'item') and .//a[contains(@href, '/lawyer/')]]",
            "//*[contains(., '@') and contains(., 'Partner')]//ancestor::*[2]",
            "//*[contains(., 'J.D.') or contains(., 'University')]//ancestor::*[position()<=3 and @class]"
        ]
        
        for pattern in content_patterns:
            try:
                elements = self.current_page.xpath(pattern)
                if elements and len(elements) >= 3:
                    logger.info(f"Found {len(elements)} directory containers with content pattern: {pattern}")
                    return elements
            except Exception:
                continue
        
        return []
    
    def _template_needs_subpage_data(self) -> bool:
        """
        Check if template has elements that require subpage navigation.
        
        Returns:
            True if template has subpage elements or education/credentials elements
        """
        # Check for elements that are typically found on individual profile pages
        subpage_indicators = [
            # Elements with subpage_elements defined
            any(hasattr(elem, 'subpage_elements') and elem.subpage_elements 
                for elem in self.template.elements),
            # Education/credentials elements (typically on individual pages)
            any(hasattr(elem, 'sub_elements') and elem.sub_elements and
                any('education' in sub.get('label', '').lower() or 'cred' in sub.get('label', '').lower()
                    for sub in elem.sub_elements if isinstance(sub, dict))
                for elem in self.template.elements if hasattr(elem, 'sub_elements')),
            # Multiple containers with different purposes (main + subpage data)
            len([elem for elem in self.template.elements if hasattr(elem, 'is_container') and elem.is_container]) >= 2,
            # Actions that navigate to specific pages
            any('link' in action.label.lower() and '/lawyer/' in getattr(action, 'target_url', '')
                for action in self.template.actions if hasattr(self.template, 'actions'))
        ]
        
        return any(subpage_indicators)
    
    def _is_subpage_container(self, element_config: ElementSelector) -> bool:
        """
        Check if a container is meant to extract data from individual subpages.
        
        Args:
            element_config: ElementSelector configuration
            
        Returns:
            True if container should extract data from subpages
        """
        subpage_indicators = [
            # Container label suggests subpage data
            any(keyword in element_config.label.lower() 
                for keyword in ['subpage', 'sublink', 'subcon', 'education', 'credential', 'experience', 'bio', 'profile']),
            # Sub-elements that are typically on individual pages
            hasattr(element_config, 'sub_elements') and element_config.sub_elements and
            any(keyword in sub.get('label', '').lower() if isinstance(sub, dict) else getattr(sub, 'label', '').lower()
                for keyword in ['education', 'credential', 'admission', 'bar', 'experience', 'bio', 'creds']
                for sub in element_config.sub_elements),
            # Template has follow_links enabled for this container
            getattr(element_config, 'follow_links', False)
        ]
        
        is_subpage = any(subpage_indicators)
        logger.debug(f"Subpage container check for '{element_config.label}': {is_subpage} (indicators: {subpage_indicators})")
        return is_subpage
    
    def _extract_subpage_container_data(self, profile_url: str, element_config: ElementSelector) -> Dict[str, Any]:
        """
        Extract container data from an individual profile subpage.
        
        Args:
            profile_url: URL of the profile page
            element_config: Container configuration for subpage extraction
            
        Returns:
            Dictionary containing extracted subpage container data
        """
        subpage_data = {}
        original_page = self.current_page
        
        try:
            logger.debug(f"Navigating to subpage for container data: {profile_url}")
            
            # Navigate to the profile page
            subpage = self._fetch_page(profile_url)
            if not subpage:
                logger.warning(f"Failed to fetch subpage: {profile_url}")
                return {}
            
            # Temporarily switch to subpage context
            self.current_page = subpage
            
            # Extract sub-elements from the subpage
            if hasattr(element_config, 'sub_elements') and element_config.sub_elements:
                for sub_element in element_config.sub_elements:
                    try:
                        if isinstance(sub_element, dict):
                            sub_label = sub_element.get('label')
                            sub_selector = sub_element.get('selector')
                            sub_type = sub_element.get('element_type', 'text')
                        else:
                            sub_label = sub_element.label
                            sub_selector = sub_element.selector
                            sub_type = sub_element.element_type
                        
                        # Enhance selector for subpage context
                        # Convert SubElement object to dict format for _map_generic_selector
                        sub_element_dict = {
                            'label': sub_label,
                            'selector': sub_selector,
                            'element_type': sub_type
                        } if not isinstance(sub_element, dict) else sub_element
                        
                        enhanced_selector = self._map_generic_selector(sub_element_dict, "profile")
                        if enhanced_selector != sub_selector:
                            logger.info(f"Enhanced subpage selector for {sub_label}: '{sub_selector}' → '{enhanced_selector}'")
                            sub_selector = enhanced_selector
                        
                        # Find elements on the subpage
                        elements = []
                        selectors_to_try = [s.strip() for s in sub_selector.split(',') if s.strip()]
                        
                        for selector_attempt in selectors_to_try:
                            try:
                                if selector_attempt.startswith('xpath:'):
                                    xpath_expr = selector_attempt[6:]
                                    elements = self.current_page.xpath(xpath_expr)
                                else:
                                    elements = self.current_page.css(selector_attempt)
                                
                                if elements:
                                    logger.debug(f"Found {len(elements)} subpage elements with selector: {selector_attempt}")
                                    break
                            except Exception:
                                continue
                        
                        # Extract data based on element type
                        if elements:
                            if sub_type == 'text' and len(elements) > 1:
                                # Multiple text elements - extract as list, skip empty/None values
                                text_values = []
                                for elem in elements:
                                    text_content = elem.text if hasattr(elem, 'text') else str(elem)
                                    if text_content and text_content.strip() and text_content.strip().lower() != 'none':
                                        text_values.append(text_content.strip())
                                # Only set if we have actual values
                                if text_values:
                                    subpage_data[sub_label] = text_values
                                else:
                                    logger.debug(f"No valid text content found for {sub_label}")
                            elif elements:
                                # Single element or first element
                                text_content = elements[0].text if hasattr(elements[0], 'text') else str(elements[0])
                                if text_content and text_content.strip() and text_content.strip().lower() != 'none':
                                    subpage_data[sub_label] = text_content.strip()
                                else:
                                    logger.debug(f"No valid text content found for {sub_label}")
                        else:
                            logger.debug(f"No elements found for subpage {sub_label} with selector {sub_selector}")
                            
                    except Exception as e:
                        logger.warning(f"Error extracting subpage element {sub_label}: {e}")
                        # Don't add None values - just skip failed extractions
            
            logger.info(f"Successfully extracted {len(subpage_data)} elements from subpage")
            
        except Exception as e:
            logger.error(f"Error during subpage container extraction: {e}")
        
        finally:
            # Restore original page context
            if original_page:
                self.current_page = original_page
        
        return subpage_data
    
    def _extract_subpage_container_data_from_main_containers(self, element_config: ElementSelector) -> List[Dict[str, Any]]:
        """
        Extract subpage container data by using profile links from main containers.
        This handles templates where subpage containers need to extract data from individual lawyer pages.
        
        Args:
            element_config: Subpage container configuration
            
        Returns:
            List of dictionaries containing extracted subpage data
        """
        containers = []
        
        try:
            logger.info(f"Subpage container '{element_config.label}' detected - extracting from individual lawyer pages")
            
            # Find main containers that should have profile links
            main_container_elements = []
            
            # Try to find directory containers using smart detection
            if self._looks_like_directory_template():
                # Use the same directory container detection as main containers
                main_container_elements = self._find_directory_containers(element_config)
                if main_container_elements:
                    logger.info(f"Found {len(main_container_elements)} main containers for subpage extraction")
            
            if not main_container_elements:
                logger.warning("No main containers found for subpage extraction")
                return []
            
            # Process each main container to extract profile links and then subpage data
            for i, container in enumerate(main_container_elements):
                try:
                    # Find profile link in this container
                    profile_link = None
                    
                    # Try to find any link within the container that points to a lawyer page
                    lawyer_links = container.css("a[href*='/lawyer/']")
                    if not lawyer_links:
                        lawyer_links = container.css("a")  # Fallback to any link
                    
                    for link in lawyer_links:
                        href = ''
                        if hasattr(link, 'get_attribute'):
                            href = link.get_attribute('href') or ''
                        elif hasattr(link, 'attrib'):
                            href = link.attrib.get('href', '')
                        
                        if href and ('/lawyer/' in href or href.startswith('/')):
                            full_url = self.current_page.urljoin(href) if href else ''
                            if full_url and '/lawyer/' in full_url:
                                profile_link = full_url
                                logger.debug(f"Found profile link for subpage container {i}: {full_url}")
                                break
                    
                    if not profile_link:
                        logger.warning(f"No profile link found for subpage container {i}")
                        containers.append({'_container_index': i, '_error': 'No profile link found'})
                        continue
                    
                    # Extract subpage data from the individual lawyer page
                    logger.info(f"Extracting subpage data from: {profile_link}")
                    subpage_data = self._extract_subpage_container_data(profile_link, element_config)
                    
                    if subpage_data:
                        subpage_data['_profile_link'] = profile_link
                        subpage_data['_container_index'] = i
                        containers.append(subpage_data)
                        logger.debug(f"Successfully extracted subpage data for container {i}: {list(subpage_data.keys())}")
                    else:
                        logger.warning(f"No subpage data extracted for container {i}")
                        containers.append({'_container_index': i, '_profile_link': profile_link, '_error': 'No data extracted'})
                
                except Exception as container_error:
                    logger.warning(f"Error processing subpage container {i}: {container_error}")
                    containers.append({'_container_index': i, '_error': str(container_error)})
            
            logger.info(f"Successfully processed {len(containers)} subpage containers")
            return containers
            
        except Exception as e:
            logger.error(f"Error extracting subpage container data from main containers: {e}")
            return []
    
    def _map_generic_selector(self, sub_element: dict, context: str = "directory") -> str:
        """
        Map generic selectors to meaningful ones based on label and context.
        
        Args:
            sub_element: Sub-element configuration dict
            context: Context type ("directory" or "profile")
            
        Returns:
            Enhanced selector string
        """
        label = sub_element.get('label', '').lower()
        original_selector = sub_element.get('selector', '')
        
        # Always preserve XPath selectors - they are position-specific and already optimized
        if original_selector.startswith('xpath:'):
            return original_selector
        
        # If selector is already specific, keep it
        if len(original_selector) > 10 and ('.' in original_selector or '#' in original_selector or '[' in original_selector):
            return original_selector
        
        # Map generic selectors based on label
        if 'name' in label:
            if context == "directory":
                return "strong, p.name strong, h3, h2, .name, [class*='name'] strong, strong:first-of-type"
            else:
                return "h1, h2, .entry-title, .page-title, .lawyer-name, .attorney-name"
                
        elif any(keyword in label for keyword in ['title', 'position', 'job']):
            if context == "directory":
                return ".title, .position, .job-title, [class*='title'], [class*='position'], p.title span:first-child, span[class*='position']"
            else:
                return ".position, .title, .job-title, h2 + p, h1 + p"
                
        elif any(keyword in label for keyword in ['email', 'mail']):
            return "a[href^='mailto:'], p.contact-details a[href^='mailto:'], .email, [class*='email'], a[href*='@']"
            
        elif 'phone' in label:
            return "a[href^='tel:'], .phone, [class*='phone'], a[href*='tel']"
            
        elif any(keyword in label for keyword in ['sector', 'practice', 'area']):
            if context == "directory":
                return ".practice-area, .sector, [class*='practice'], [class*='sector'], p.contact-details span:not([class*='position']), p.title span:last-child, span[class*='practice']"
            else:
                return ".practice-area, .capabilities, .focus-areas, a[href*='/practice/']"
                
        elif any(keyword in label for keyword in ['link', 'profile', 'url']) or 'email' in label:
            if 'email' in label:
                return "a[href^='mailto:'], .email, [class*='email'], a[href*='@']"
            else:
                return "a[href*='/lawyer/'], a[href*='/attorney/'], a[href*='/people/'], a[href*='/team/'], a"
            
        elif 'education' in label:
            # More specific selectors for education to avoid duplication
            if 'education2' in label:
                return ".education li:nth-of-type(n+2), ul[class*='education'] li:nth-of-type(n+2)"
            else:
                return ".education li:first-child, .education li:first-of-type, ul[class*='education'] li:first-child"
            
        elif any(keyword in label for keyword in ['cred', 'admission', 'bar']):
            # More specific selectors for credentials to avoid duplication
            if 'creds2' in label:
                return ".admissions li:nth-of-type(n+2), .bar li:nth-of-type(n+2), .credentials li:nth-of-type(n+2)"
            else:
                return ".admissions li:first-child, .bar li:first-child, .credentials li:first-child"
        
        # Default fallback - return original
        return original_selector
    
    def _initialize_fetcher(self) -> None:
        """Initialize the appropriate Scrapling fetcher based on template settings."""
        try:
            # Initialize PlayWrightFetcher with proper configuration
            self.fetcher = PlayWrightFetcher
            # Configure the fetcher class with the new API (headless is not a configure parameter)
            PlayWrightFetcher.configure(auto_match=True)
            # Create a single fetcher instance that will be reused for all page fetches
            self.fetcher_instance = PlayWrightFetcher()
            logger.info("Initialized PlayWrightFetcher instance for browser session reuse with AutoMatch enabled")
                
        except Exception as e:
            logger.error(f"Failed to initialize fetcher: {e}")
            raise
    
    def _fetch_page(self, url: str) -> Optional[Adaptor]:
        """
        Fetch a page using the configured fetcher.
        
        Args:
            url: URL to fetch
            
        Returns:
            Adaptor instance for the fetched page or None if failed
        """
        try:
            logger.info(f"Fetching page: {url}")
            
            # Configure fetcher options
            fetch_options = {
                'network_idle': True,
                'timeout': self.template.wait_timeout * 1000,  # Convert to milliseconds
                'headless': self.template.headless
            }
            
            # Use the single fetcher instance to reuse browser session
            if self.fetcher_instance:
                page = self.fetcher_instance.fetch(url, **fetch_options)
            else:
                # Fallback to class method if instance not available
                logger.warning("Using class method fetch - browser session not reused")
                page = self.fetcher.fetch(url, **fetch_options)
            
            if page and page.status == 200:
                logger.info(f"Successfully fetched page. Status: {page.status}")
                # Track the page for cleanup
                self.browser_pages.append(page)
                return page
            else:
                logger.error(f"Page fetch failed. Status: {page.status if page else 'None'}")
                return None
                
        except Exception as e:
            logger.error(f"Error fetching page {url}: {e}")
            return None
    
    def _format_cookies_for_scrapling(self) -> List[Dict[str, Any]]:
        """Format cookies from template for Scrapling."""
        cookies = []
        for cookie in self.template.cookies:
            cookie_dict = {
                'name': cookie.name,
                'value': cookie.value,
                'domain': cookie.domain,
                'path': cookie.path
            }
            if hasattr(cookie, 'secure'):
                cookie_dict['secure'] = cookie.secure
            if hasattr(cookie, 'httpOnly'):
                cookie_dict['httpOnly'] = cookie.httpOnly
            
            cookies.append(cookie_dict)
        
        return cookies
    
    def _extract_data(self) -> Dict[str, Any]:
        """
        Extract data from the page using template selectors.
        
        Returns:
            Dictionary containing extracted data
        """
        extracted_data = {}
        
        # Initialize progress tracker
        progress = ProgressTracker(len(self.template.elements), "Extracting elements")
        
        for idx, element in enumerate(self.template.elements):
            try:
                progress.update(0, f"Extracting: {element.label}")
                logger.debug(f"Extracting data for element: {element.label}")
                
                value = self._extract_element_data(element)
                extracted_data[element.label] = value
                
                logger.debug(f"Successfully extracted {element.label}: {str(value)[:100]}...")
                progress.update(1, f"Completed: {element.label}")
                
            except Exception as e:
                error_msg = f"Failed to extract {element.label}: {str(e)}"
                logger.warning(error_msg)
                progress.update(1, f"Failed: {element.label}")
                
                if element.is_required:
                    # Check if this is a location-specific element that might not exist on all profiles
                    is_location_element = (
                        'location' in element.label.lower() or
                        'city' in element.label.lower() or
                        'normalize-space' in element.selector.lower() or
                        any(city in element.selector for city in ['Riyadh', 'Dubai', 'London', 'New York'])
                    )
                    
                    if is_location_element:
                        logger.warning(f"Location-specific required element failed, continuing: {element.label}")
                        extracted_data[element.label] = None
                    else:
                        raise Exception(error_msg)
                else:
                    extracted_data[element.label] = None
        
        progress.finish("Extraction complete")
        
        # Post-process to merge related containers with subpage data
        merged_data = self._merge_related_containers(extracted_data)
        return merged_data
    
    def _merge_related_containers(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge related containers that share the same profile link into nested structures.
        
        Args:
            extracted_data: Raw extracted data with separate containers
            
        Returns:
            Merged data with subpage information nested within main containers
        """
        try:
            # Find main containers and subpage containers
            main_containers = {}
            subpage_containers = {}
            other_data = {}
            
            for label, data in extracted_data.items():
                if isinstance(data, list) and data and isinstance(data[0], dict):
                    # Check if this looks like a main container (has basic profile info)
                    sample_item = data[0]
                    has_basic_fields = any(field in sample_item for field in ['name', 'Position', 'title'])
                    has_subpage_fields = any(field in sample_item for field in ['education', 'credential', 'admission', 'bar'])
                    
                    if has_basic_fields and not has_subpage_fields:
                        main_containers[label] = data
                        logger.info(f"Identified '{label}' as main container with {len(data)} items")
                    elif has_subpage_fields or 'subpage' in label.lower() or 'second' in label.lower():
                        subpage_containers[label] = data
                        logger.info(f"Identified '{label}' as subpage container with {len(data)} items")
                    else:
                        other_data[label] = data
                else:
                    other_data[label] = data
            
            # If we have both main and subpage containers, merge them
            if main_containers and subpage_containers:
                logger.info("Merging main and subpage containers...")
                
                for main_label, main_data in main_containers.items():
                    for main_item in main_data:
                        if not isinstance(main_item, dict) or '_profile_link' not in main_item:
                            continue
                            
                        main_profile_link = main_item['_profile_link']
                        main_index = main_item.get('_container_index')
                        
                        # Find matching subpage data
                        for subpage_label, subpage_data in subpage_containers.items():
                            for subpage_item in subpage_data:
                                if not isinstance(subpage_item, dict):
                                    continue
                                    
                                subpage_profile_link = subpage_item.get('_profile_link')
                                subpage_index = subpage_item.get('_container_index')
                                
                                # Match by profile link or container index
                                if (main_profile_link and subpage_profile_link and main_profile_link == subpage_profile_link) or \
                                   (main_index is not None and subpage_index is not None and main_index == subpage_index):
                                    
                                    # Merge subpage data into main item
                                    subpage_info = {k: v for k, v in subpage_item.items() 
                                                  if not k.startswith('_')}
                                    
                                    # Nest subpage data under a clear key, or merge directly if it's profile details
                                    if subpage_info:
                                        # For cleaner structure, merge certain fields directly
                                        if 'education' in subpage_label.lower() or 'credential' in subpage_label.lower():
                                            # Add education/credentials directly to the main profile
                                            for key, value in subpage_info.items():
                                                if 'education' in key.lower() or 'credential' in key.lower():
                                                    main_item[key] = value
                                                else:
                                                    main_item[f'{key}'] = value
                                        else:
                                            # For other subpage data, nest under descriptive key
                                            clean_label = subpage_label.replace('container', '').replace('second', '').strip()
                                            main_item[f'{clean_label}_details'] = subpage_info
                                        
                                        logger.debug(f"Merged {subpage_label} data into {main_label} item {main_index}")
                
                # Return merged data (main containers + other data, exclude processed subpage containers)
                merged_result = {**main_containers, **other_data}
                logger.info(f"Successfully merged containers. Final structure: {list(merged_result.keys())}")
                return merged_result
            
            # If no merging needed, return original data
            return extracted_data
            
        except Exception as e:
            logger.warning(f"Error merging containers: {e}")
            return extracted_data
    
    def _auto_scroll_to_load_all_content(self, target_page=None):
        """
        Automatically scroll to load all content on pages with infinite scroll.
        
        Args:
            target_page: Specific page to scroll on. If None, uses self.current_page
        """
        try:
            # Use the specified page or current page
            scroll_page = target_page or self.current_page
            if not scroll_page:
                logger.warning("No page available for auto-scroll")
                return
            # Count initial elements
            initial_count = self._count_profile_elements()
            logger.info(f"Initial profile count: {initial_count}")
            
            if initial_count == 0:
                logger.warning("No profiles found initially, skipping auto-scroll")
                return
            
            max_scrolls = 20  # Prevent infinite loops
            scroll_count = 0
            last_count = initial_count
            stable_count = 0  # Track how many times count stayed the same
            
            # Use Scrapling's page_action to scroll
            progress = ProgressTracker(max_scrolls, "Auto-scrolling to load content")
            
            while scroll_count < max_scrolls:
                scroll_count += 1
                progress.update(0, f"Scroll {scroll_count}: {last_count} profiles loaded")
                
                try:
                    # Try different approaches to load more content
                    try:
                        # First, try to find and click "Load More" buttons
                        load_more_success = self._try_load_more_buttons()
                        
                        if not load_more_success:
                            # Try scrolling using Scrapling's internal page
                            scroll_success = self._try_scrapling_scroll(scroll_page)
                            
                            if not scroll_success:
                                logger.warning("Cannot access page for scrolling, trying pagination")
                                # Check for pagination links as fallback
                                pagination_success = self._try_pagination_load()
                                
                                if not pagination_success:
                                    logger.info("No more content loading methods available")
                                    break
                    except Exception as scroll_method_error:
                        logger.debug(f"Scroll method failed: {scroll_method_error}")
                        # If scrolling fails, we might have reached the end or page changed
                        stable_count += 1
                        if stable_count >= 2:
                            logger.info("Scroll failed multiple times, assuming end of content reached")
                            break
                    
                    # Wait for content to load
                    time.sleep(2)
                    
                    # Check if more content loaded
                    current_count = self._count_profile_elements()
                    logger.debug(f"After scroll {scroll_count}: {current_count} profiles")
                    
                    if current_count > last_count:
                        logger.info(f"Loaded {current_count - last_count} more profiles (total: {current_count})")
                        last_count = current_count
                        stable_count = 0  # Reset stability counter
                        progress.update(1, f"Found {current_count} profiles")
                    else:
                        stable_count += 1
                        progress.update(1, f"No new content ({stable_count}/3)")
                        
                        # If count has been stable for 3 scrolls, we've reached the end
                        if stable_count >= 3:
                            logger.info(f"No new content after {stable_count} scrolls - reached end")
                            break
                
                except Exception as scroll_error:
                    logger.warning(f"Error during scroll {scroll_count}: {scroll_error}")
                    progress.update(1, f"Scroll error {scroll_count}")
                    break
            
            final_count = self._count_profile_elements()
            progress.finish(f"Loaded {final_count} total profiles")
            logger.info(f"Auto-scroll complete: {final_count} profiles loaded (started with {initial_count})")
            
        except Exception as e:
            logger.error(f"Error during auto-scroll: {e}")
    
    def _count_profile_elements(self) -> int:
        """
        Count the number of profile elements currently on the page.
        """
        try:
            # Try multiple selectors to count profiles
            selectors_to_try = [
                '.people-list strong',  # Names
                '.wp-grid-builder .wpgb-card',  # Grid cards
                '.people-list .wp-block-column',  # Column blocks
                '[class*="people"] strong',  # Any people container with strong tags
                '.wpgb-grid-archivePeople > div'  # Direct children of people grid
            ]
            
            max_count = 0
            for selector in selectors_to_try:
                try:
                    elements = self.current_page.css(selector)
                    count = len(elements) if elements else 0
                    max_count = max(max_count, count)
                    logger.debug(f"Selector '{selector}': {count} elements")
                except Exception:
                    continue
            
            return max_count
            
        except Exception as e:
            logger.warning(f"Error counting profile elements: {e}")
            return 0
    
    def _extract_element_data(self, element: ElementSelector) -> Any:
        """
        Extract data from a single element using multiple fallback strategies.
        
        Args:
            element: ElementSelector defining what to extract
            
        Returns:
            Extracted data (string, list, or None)
        """
        if not self.current_page:
            raise Exception("No page available for data extraction")
        
        try:
            # Handle container elements specially
            if hasattr(element, 'is_container') and element.is_container:
                return self._extract_container_data(element)
            
            # Try multiple selection strategies in order of reliability
            found_elements = self._find_elements_with_fallback(element)
            
            if not found_elements:
                if element.is_required:
                    # Check if this is a location-specific element that might not exist on all profiles
                    is_location_element = (
                        'location' in element.label.lower() or
                        'city' in element.label.lower() or
                        'normalize-space' in element.selector.lower() or
                        any(city in element.selector for city in ['Riyadh', 'Dubai', 'London', 'New York'])
                    )
                    
                    if is_location_element:
                        logger.warning(f"Location-specific required element not found, continuing: {element.label}")
                        return None
                    else:
                        raise Exception(f"Required element not found: {element.selector}")
                return None
            
            # Handle multiple elements
            if element.is_multiple:
                return self._extract_multiple_elements(found_elements, element)
            else:
                return self._extract_single_element(found_elements[0], element)
                
        except Exception as e:
            logger.error(f"Error extracting element {element.label}: {e}")
            raise
    
    def _find_elements_with_fallback(self, element: ElementSelector) -> List:
        """
        Find elements using multiple strategies for maximum reliability.
        
        Args:
            element: ElementSelector configuration
            
        Returns:
            List of found elements
        """
        strategies = [
            # Strategy 1: AutoMatch with original selector
            lambda: self._try_automatch_selector(element),
            # Strategy 2: Semantic XPath generation
            lambda: self._try_semantic_xpath(element),
            # Strategy 3: Content-based selection
            lambda: self._try_content_based_selection(element),
            # Strategy 4: Structure-based selection
            lambda: self._try_structure_based_selection(element),
            # Strategy 5: Original selector as fallback
            lambda: self._try_original_selector(element)
        ]
        
        for i, strategy in enumerate(strategies):
            try:
                elements = strategy()
                if elements:
                    logger.debug(f"Found elements using strategy {i+1} for {element.label}")
                    return elements
            except Exception as e:
                logger.debug(f"Strategy {i+1} failed for {element.label}: {e}")
                continue
        
        return []
    
    def _try_automatch_selector(self, element: ElementSelector) -> List:
        """Try AutoMatch with the original selector."""
        # Auto-detect selector type
        is_xpath = (element.selector_type == 'xpath' or 
                   element.selector.startswith('xpath:') or
                   element.selector.startswith('//') or
                   'normalize-space' in element.selector)
        
        if is_xpath:
            # Handle XPath selectors
            xpath_expr = element.selector[6:] if element.selector.startswith('xpath:') else element.selector
            # Fix common XPath syntax errors
            if 'normalize-space' in xpath_expr and not xpath_expr.startswith('//'):
                xpath_expr = '//' + xpath_expr.lstrip('/')
            
            try:
                return self.current_page.xpath(xpath_expr)
            except Exception as e:
                logger.debug(f"XPath failed: {e}")
                return []
        else:
            return self.current_page.css(element.selector)
    
    def _try_semantic_xpath(self, element: ElementSelector) -> List:
        """Generate semantic XPath based on element context."""
        # Generate XPaths based on common patterns
        semantic_xpaths = self._generate_semantic_xpaths(element)
        
        for xpath in semantic_xpaths:
            try:
                elements = self.current_page.xpath(xpath)
                if elements:
                    return elements
            except Exception:
                continue
        return []
    
    def _try_content_based_selection(self, element: ElementSelector) -> List:
        """Try to find elements based on content patterns."""
        if element.element_type == 'text' and hasattr(element, 'expected_pattern'):
            # Look for elements containing expected text patterns
            content_xpaths = [
                f"//*[contains(text(), '{pattern}')]" 
                for pattern in getattr(element, 'expected_patterns', [])
            ]
            
            for xpath in content_xpaths:
                try:
                    elements = self.current_page.xpath(xpath)
                    if elements:
                        return elements
                except Exception:
                    continue
        return []
    
    def _try_structure_based_selection(self, element: ElementSelector) -> List:
        """Try to find elements based on structural patterns."""
        structure_xpaths = self._generate_structure_xpaths(element)
        
        for xpath in structure_xpaths:
            try:
                elements = self.current_page.xpath(xpath)
                if elements:
                    return elements
            except Exception:
                continue
        return []
    
    def _try_original_selector(self, element: ElementSelector) -> List:
        """Fallback to original selector without AutoMatch."""
        try:
            # Auto-detect selector type
            is_xpath = (element.selector_type == 'xpath' or 
                       element.selector.startswith('xpath:') or
                       element.selector.startswith('//') or
                       'normalize-space' in element.selector)
            
            if is_xpath:
                # Handle XPath selectors
                xpath_expr = element.selector[6:] if element.selector.startswith('xpath:') else element.selector
                # Fix common XPath syntax errors
                if 'normalize-space' in xpath_expr and not xpath_expr.startswith('//'):
                    xpath_expr = '//' + xpath_expr.lstrip('/')
                return self.current_page.xpath(xpath_expr)
            else:
                return self.current_page.css(element.selector)
        except Exception:
            pass
        return []
    
    def _generate_semantic_xpaths(self, element: ElementSelector) -> List[str]:
        """
        Generate semantic XPaths based on element label and context.
        
        Args:
            element: ElementSelector configuration
            
        Returns:
            List of semantic XPath expressions
        """
        xpaths = []
        label = element.label.lower()
        
        # Common patterns based on label
        if 'name' in label:
            xpaths.extend([
                "//h1[contains(@class, 'name')]",
                "//h2[contains(@class, 'name')]", 
                "//span[contains(@class, 'name')]",
                "//*[@id[contains(., 'name')]]",
                "//strong[position()=1]",
                "//*[contains(@class, 'person-name') or contains(@class, 'lawyer-name')]"
            ])
        elif 'email' in label:
            xpaths.extend([
                "//a[contains(@href, 'mailto:')]",
                "//*[contains(@class, 'email')]",
                "//*[contains(text(), '@') and contains(text(), '.')]"
            ])
        elif 'phone' in label:
            xpaths.extend([
                "//a[contains(@href, 'tel:')]",
                "//*[contains(@class, 'phone')]",
                "//*[contains(text(), '(') and contains(text(), ')')]"
            ])
        elif 'title' in label or 'position' in label:
            xpaths.extend([
                "//*[contains(@class, 'title')]",
                "//*[contains(@class, 'position')]",
                "//*[contains(@class, 'job-title')]"
            ])
        elif 'link' in label or 'url' in label:
            xpaths.extend([
                "//a[@href]",
                "//*[@href]"
            ])
        
        return xpaths
    
    def _generate_structure_xpaths(self, element: ElementSelector) -> List[str]:
        """
        Generate structure-based XPaths.
        
        Args:
            element: ElementSelector configuration
            
        Returns:
            List of structure-based XPath expressions
        """
        xpaths = []
        
        # Convert CSS to XPath if needed
        if element.selector_type == 'css':
            xpath_equiv = self._css_to_xpath(element.selector)
            if xpath_equiv:
                xpaths.append(xpath_equiv)
        
        # Generate hierarchical patterns
        if element.selector_type == 'css' and '>' in element.selector:
            parts = element.selector.split('>')
            if len(parts) >= 2:
                # Try more flexible hierarchical patterns
                parent_part = parts[-2].strip()
                child_part = parts[-1].strip()
                xpaths.extend([
                    f"//*[contains(@class, '{parent_part.replace('.', '')}')]//*[contains(@class, '{child_part.replace('.', '')}')]",
                    f"//{parent_part.replace('.', '')}//{child_part.replace('.', '')}"
                ])
        
        return xpaths
    
    def _css_to_xpath(self, css_selector: str) -> str:
        """
        Convert simple CSS selectors to XPath.
        
        Args:
            css_selector: CSS selector string
            
        Returns:
            Equivalent XPath expression
        """
        # Basic CSS to XPath conversion
        import re
        
        xpath = css_selector
        
        # Convert class selectors
        xpath = re.sub(r'\.([a-zA-Z0-9_-]+)', r"[contains(@class, '\1')]", xpath)
        
        # Convert ID selectors
        xpath = re.sub(r'#([a-zA-Z0-9_-]+)', r"[@id='\1']", xpath)
        
        # Convert descendant combinators
        xpath = xpath.replace(' ', '//')
        
        # Convert child combinators
        xpath = xpath.replace('>', '/')
        
        # Add root if not present
        if not xpath.startswith('//') and not xpath.startswith('/'):
            xpath = '//' + xpath
        
        return xpath
    
    def _extract_single_element(self, element, element_config: ElementSelector) -> Any:
        """Extract data from a single element based on its type."""
        try:
            if element_config.element_type == 'text':
                return element.text.strip() if hasattr(element, 'text') else str(element).strip()
            
            elif element_config.element_type == 'html':
                return str(element) if hasattr(element, '__str__') else element.get_attribute('outerHTML')
            
            elif element_config.element_type == 'link':
                if hasattr(element, 'attrib'):
                    return element.attrib.get('href', '')
                else:
                    return element.get_attribute('href') or ''
            
            elif element_config.element_type == 'attribute':
                attr_name = element_config.attribute_name or 'value'
                if hasattr(element, 'attrib'):
                    return element.attrib.get(attr_name, '')
                else:
                    return element.get_attribute(attr_name) or ''
            
            else:
                # Default to text
                return element.text.strip() if hasattr(element, 'text') else str(element).strip()
                
        except Exception as e:
            logger.warning(f"Error extracting single element: {e}")
            return None
    
    def _extract_container_data(self, element_config: ElementSelector) -> List[Dict[str, Any]]:
        """
        Extract data from container elements using AutoMatch and find_similar.
        
        Args:
            element_config: ElementSelector with container configuration
            
        Returns:
            List of dictionaries containing extracted data from each container
        """
        containers = []
        
        try:
            logger.info(f"Extracting container data with selector: {element_config.selector}")
            
            # Check if this is a subpage container that should extract data from individual pages
            if self._is_subpage_container(element_config):
                return self._extract_subpage_container_data_from_main_containers(element_config)
            
            # Smart container detection for directory pages
            container_elements = []
            
            # If this looks like a directory template, FORCE smart container detection
            if self._looks_like_directory_template():
                logger.info("Directory template detected - FORCING smart container detection")
                container_elements = self._find_directory_containers(element_config)
                if container_elements:
                    logger.info(f"Smart directory detection found {len(container_elements)} lawyer profile containers")
                    logger.info(f"OVERRIDING template selector '{element_config.selector}' with smart detection results")
                    # Skip the original selector entirely for directory templates
                else:
                    logger.warning("Smart directory detection failed, falling back to original selector")
            
            # Fallback to original selector approach ONLY if not a directory template
            if not container_elements:
                try:
                    # Auto-detect selector type if mismatch or xpath prefix
                    is_xpath = (element_config.selector_type == 'xpath' or 
                               element_config.selector.startswith('xpath:') or
                               element_config.selector.startswith('//') or
                               'normalize-space' in element_config.selector)
                    
                    if is_xpath:
                        # Handle XPath selectors
                        xpath_expr = element_config.selector[6:] if element_config.selector.startswith('xpath:') else element_config.selector
                        # Fix common XPath syntax errors
                        if xpath_expr.startswith('//a[') and 'normalize-space' in xpath_expr:
                            # Already valid XPath
                            pass
                        elif 'normalize-space' in xpath_expr and not xpath_expr.startswith('//'):
                            # Fix missing // prefix
                            xpath_expr = '//' + xpath_expr.lstrip('/')
                        
                        try:
                            container_elements = self.current_page.xpath(xpath_expr)
                        except Exception as e:
                            logger.warning(f"XPath failed: {e}")
                            container_elements = []
                        if container_elements:
                            logger.info(f"XPath AutoMatch found {len(container_elements)} elements with selector: {xpath_expr}")
                    else:
                        # Handle CSS selectors
                        container_elements = self.current_page.css(element_config.selector)
                        if container_elements:
                            logger.info(f"CSS AutoMatch found {len(container_elements)} elements with selector: {element_config.selector}")
                except Exception as automatch_error:
                    logger.debug(f"AutoMatch failed: {automatch_error}")
            
            # If AutoMatch didn't work, try regular selection
            if not container_elements:
                try:
                    # Auto-detect selector type if mismatch or xpath prefix
                    is_xpath = (element_config.selector_type == 'xpath' or 
                               element_config.selector.startswith('xpath:') or
                               element_config.selector.startswith('//') or
                               'normalize-space' in element_config.selector)
                    
                    if is_xpath:
                        # Handle XPath selectors
                        xpath_expr = element_config.selector[6:] if element_config.selector.startswith('xpath:') else element_config.selector
                        # Fix common XPath syntax errors
                        if xpath_expr.startswith('//a[') and 'normalize-space' in xpath_expr:
                            # Already valid XPath
                            pass
                        elif 'normalize-space' in xpath_expr and not xpath_expr.startswith('//'):
                            # Fix missing // prefix
                            xpath_expr = '//' + xpath_expr.lstrip('/')
                        
                        container_elements = self.current_page.xpath(xpath_expr)
                        if container_elements:
                            logger.info(f"Regular XPath found {len(container_elements)} elements with selector: {xpath_expr}")
                    else:
                        # Handle CSS selectors
                        container_elements = self.current_page.css(element_config.selector)
                        if container_elements:
                            logger.info(f"Regular CSS found {len(container_elements)} elements with selector: {element_config.selector}")
                except Exception as selection_error:
                    logger.debug(f"Regular selection failed: {selection_error}")
            
            # Try without dots for class selectors (handle dynamic classes)
            if not container_elements and '.' in element_config.selector:
                try:
                    # Handle cases like ".people.loading" -> "[class*='people'][class*='loading']"
                    classes = element_config.selector.replace('.', '').split()
                    if len(classes) > 1:
                        class_selector = ''.join([f'[class*="{cls}"]' for cls in classes])
                        container_elements = self.current_page.css(class_selector)
                        if container_elements:
                            logger.info(f"Class fallback found {len(container_elements)} elements with: {class_selector}")
                except Exception as class_error:
                    logger.debug(f"Class fallback failed: {class_error}")
                
            # CRITICAL: If use_find_similar is True, find all similar containers
            if hasattr(element_config, 'use_find_similar') and element_config.use_find_similar and container_elements:
                logger.info("Using find_similar to locate all matching containers")
                try:
                    # Use Scrapling's find_similar method to find all similar containers
                    similar_containers = container_elements[0].find_similar()
                    if similar_containers:
                        container_elements = similar_containers
                        logger.info(f"find_similar found {len(container_elements)} similar containers")
                except Exception as find_similar_error:
                    logger.warning(f"find_similar failed: {find_similar_error}")
                    # Continue with the original elements
                
            # If no elements found with direct selector, try alternative approaches
            if not container_elements:
                logger.warning(f"No elements found with selector: {element_config.selector}")
                
                # Convert nth-child selectors to generic ones for finding all similar elements
                generic_selector = self._make_selector_generic(element_config.selector)
                if generic_selector != element_config.selector:
                    logger.info(f"Trying generic selector: {generic_selector}")
                    try:
                        container_elements = self.current_page.css(generic_selector)
                        if container_elements:
                            logger.info(f"Generic selector found {len(container_elements)} elements")
                    except Exception:
                        container_elements = self.current_page.css(generic_selector)
                
                # Try AutoMatch with pattern-based selectors
                if not container_elements:
                    pattern_selectors = self._generate_pattern_selectors(element_config.selector)
                    for pattern_sel in pattern_selectors:
                        try:
                            container_elements = self.current_page.css(pattern_sel)
                            if container_elements:
                                logger.info(f"AutoMatch pattern found {len(container_elements)} elements with: {pattern_sel}")
                                break
                        except Exception:
                            continue
                
                # Fallback to manual alternative selectors
                if not container_elements:
                    # Try without dots in case selector has class issues
                    if element_config.selector.startswith('.'):
                        alt_selector = element_config.selector[1:]  # Remove leading dot
                        logger.info(f"Trying alternative selector: {alt_selector}")
                        container_elements = self.current_page.css(f'[class*="{alt_selector}"]')
                    
                    # If still no elements, try xpath
                    if not container_elements:
                        try:
                            xpath_selector = f"//*[contains(@class, '{element_config.selector.replace('.', '').replace(' ', '')}')]"
                            logger.info(f"Trying XPath selector: {xpath_selector}")
                            container_elements = self.current_page.xpath(xpath_selector)
                        except Exception as xpath_error:
                            logger.warning(f"XPath selection failed: {xpath_error}")
            
            # Final fallback: if XPath fails, try intelligent directory detection
            if not container_elements and (element_config.selector_type == 'xpath' or element_config.selector.startswith('xpath:')):
                logger.info("XPath selector found no elements, trying intelligent fallback for directory pages")
                try:
                    # Try common directory container patterns
                    fallback_selectors = [
                        '.wp-grid-builder.wpgb-template.wpgb-grid-archivePeople div.wp-block-column',
                        '.people-list div.wp-block-column',
                        '[class*="people"] div.wp-block-column',
                        '[class*="grid"] div.wp-block-column',
                        'div.wp-block-column',  # Very broad fallback
                    ]
                    
                    for fallback_selector in fallback_selectors:
                        try:
                            container_elements = self.current_page.css(fallback_selector)
                            if container_elements and len(container_elements) > 5:  # Reasonable number for directory
                                logger.info(f"Fallback selector found {len(container_elements)} elements: {fallback_selector}")
                                break
                        except Exception:
                            continue
                            
                except Exception as fallback_error:
                    logger.debug(f"Fallback directory detection failed: {fallback_error}")
            
            if not container_elements:
                logger.warning(f"No container elements found after trying multiple selectors for: {element_config.selector}")
                return []
            
            logger.info(f"Found {len(container_elements)} container elements")
            
            # Count total sub-elements to extract for better progress tracking
            total_sub_elements = len(container_elements)
            if hasattr(element_config, 'sub_elements') and element_config.sub_elements:
                total_sub_elements = len(container_elements) * len(element_config.sub_elements)
            
            # Initialize progress tracker for actual sub-elements
            progress = ProgressTracker(total_sub_elements, f"Extracting from {element_config.label}")
            extracted_count = 0
            
            # Extract data from each container
            for i, container in enumerate(container_elements):
                progress.update(0, f"Processing person {i+1}/{len(container_elements)}")
                container_data = {}
                
                try:
                    # Extract sub-elements if defined
                    if hasattr(element_config, 'sub_elements') and element_config.sub_elements:
                        profile_link = None  # Track profile link for subpage navigation
                        
                        # Check if we need to extract profile links for subpage navigation
                        needs_profile_link = self._template_needs_subpage_data()
                        
                        for sub_element in element_config.sub_elements:
                            try:
                                # Handle both dict and object sub-elements
                                if isinstance(sub_element, dict):
                                    sub_label = sub_element.get('label')
                                    sub_selector = sub_element.get('selector')
                                    sub_type = sub_element.get('element_type', 'text')
                                    sub_required = sub_element.get('is_required', True)
                                else:
                                    sub_label = sub_element.label
                                    sub_selector = sub_element.selector
                                    sub_type = sub_element.element_type
                                    sub_required = getattr(sub_element, 'is_required', True)
                                
                                extracted_count += 1
                                progress.update(0, f"Extracting {sub_label} from person {i+1}")
                                logger.debug(f"Extracting sub-element {sub_label} with selector: {sub_selector}")
                                
                                # Enhance generic selectors with smart mapping
                                if isinstance(sub_element, dict):
                                    enhanced_selector = self._map_generic_selector(sub_element, "directory")
                                    if enhanced_selector != sub_selector:
                                        logger.info(f"Enhanced selector for {sub_label}: '{sub_selector}' → '{enhanced_selector}'")
                                        sub_selector = enhanced_selector
                                
                                # Handle special case for empty selector (extract from container itself)
                                if not sub_selector:
                                    # Extract directly from the container element
                                    if sub_type == 'text':
                                        text_content = container.text if hasattr(container, 'text') else str(container)
                                        container_data[sub_label] = text_content.strip() if text_content else ''
                                    elif sub_type == 'link':
                                        href = ''
                                        if hasattr(container, 'get_attribute'):
                                            href = container.get_attribute('href') or ''
                                        elif hasattr(container, 'attrib'):
                                            href = container.attrib.get('href', '')
                                        full_url = self.current_page.urljoin(href) if href else ''
                                        container_data[sub_label] = full_url
                                        
                                        # Store link for potential subpage navigation
                                        if full_url and (hasattr(element_config, 'follow_links') and element_config.follow_links):
                                            container_data['_profile_link'] = full_url
                                    elif sub_type == 'attribute':
                                        attr_name = sub_element.get('attribute_name', 'value') if isinstance(sub_element, dict) else getattr(sub_element, 'attribute_name', 'value')
                                        attr_value = ''
                                        if hasattr(container, 'get_attribute'):
                                            attr_value = container.get_attribute(attr_name) or ''
                                        elif hasattr(container, 'attrib'):
                                            attr_value = container.attrib.get(attr_name, '')
                                        container_data[sub_label] = attr_value
                                    continue
                                
                                # Find sub-element within this container
                                sub_elements = []
                                if sub_selector:
                                    # Try multiple selectors if comma-separated
                                    selectors_to_try = [s.strip() for s in sub_selector.split(',') if s.strip()]
                                    
                                    for selector_attempt in selectors_to_try:
                                        try:
                                            # Handle XPath selectors
                                            if selector_attempt.startswith('xpath:'):
                                                xpath_expr = selector_attempt[6:]  # Remove 'xpath:' prefix
                                                logger.debug(f"Using XPath selector: {xpath_expr}")
                                                sub_elements = container.xpath(xpath_expr)
                                            else:
                                                # Handle CSS selectors
                                                sub_elements = container.css(selector_attempt)
                                            
                                            if sub_elements:
                                                logger.debug(f"Successfully found {len(sub_elements)} elements with selector: {selector_attempt}")
                                                break
                                                
                                        except Exception as selector_error:
                                            logger.debug(f"Selector '{selector_attempt}' failed for {sub_label}: {selector_error}")
                                            continue
                                    
                                    # If no enhanced selectors worked, try original approach
                                    if not sub_elements:
                                        try:
                                            original_selector = sub_element.get('selector') if isinstance(sub_element, dict) else sub_element.selector
                                            if original_selector and original_selector != sub_selector:
                                                if original_selector.startswith('xpath:'):
                                                    xpath_expr = original_selector[6:]
                                                    sub_elements = container.xpath(xpath_expr)
                                                else:
                                                    sub_elements = container.css(original_selector)
                                                if sub_elements:
                                                    logger.debug(f"Original selector worked: {original_selector}")
                                        except Exception:
                                            pass
                                    
                                    # Final fallback to enhanced XPath patterns
                                    if not sub_elements and any(keyword in sub_label.lower() for keyword in ['name', 'email', 'phone', 'title', 'position']):
                                        fallback_xpaths = self._generate_fallback_xpaths(sub_label, sub_type)
                                        for fallback_xpath in fallback_xpaths:
                                            try:
                                                sub_elements = container.xpath(fallback_xpath)
                                                if sub_elements:
                                                    # Validate that we got reasonable content
                                                    valid_elements = self._validate_extracted_elements(sub_elements, sub_label, sub_type)
                                                    if valid_elements:
                                                        sub_elements = valid_elements
                                                        logger.info(f"Fallback XPath worked for {sub_label}: {fallback_xpath}")
                                                        break
                                                    else:
                                                        sub_elements = []  # Reset if validation failed
                                            except Exception:
                                                continue
                                
                                if sub_elements:
                                    try:
                                        if sub_type == 'text':
                                            # Check if we should extract multiple values (for fields like sector, practice areas)
                                            should_extract_multiple = (
                                                len(sub_elements) > 1 and 
                                                any(keyword in sub_label.lower() for keyword in ['sector', 'practice', 'area', 'expertise'])
                                            )
                                            
                                            if should_extract_multiple:
                                                # Extract all elements as array
                                                values = []
                                                for elem in sub_elements:
                                                    text_content = elem.text if hasattr(elem, 'text') else str(elem)
                                                    if text_content and text_content.strip():
                                                        values.append(text_content.strip())
                                                container_data[sub_label] = values
                                            else:
                                                # Extract single value
                                                text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                                # Always normalize sector/practice fields to arrays for consistency
                                                if any(keyword in sub_label.lower() for keyword in ['sector', 'practice', 'area', 'expertise']):
                                                    container_data[sub_label] = [text_content.strip()] if text_content and text_content.strip() else []
                                                else:
                                                    container_data[sub_label] = text_content.strip() if text_content else ''
                                        elif sub_type == 'link':
                                            href = ''
                                            if hasattr(sub_elements[0], 'get_attribute'):
                                                href = sub_elements[0].get_attribute('href') or ''
                                            elif hasattr(sub_elements[0], 'attrib'):
                                                href = sub_elements[0].attrib.get('href', '')
                                            full_url = self.current_page.urljoin(href) if href else ''
                                            container_data[sub_label] = full_url
                                            
                                            # Store profile link for subpage navigation
                                            if full_url and any(keyword in sub_label.lower() for keyword in ['profile', 'link', 'url', 'page']):
                                                profile_link = full_url
                                                container_data['_profile_link'] = full_url
                                                logger.debug(f"Found profile link for container {i}: {full_url}")
                                            
                                            # Store link for potential subpage navigation (legacy support)
                                            elif full_url and (hasattr(element_config, 'follow_links') and element_config.follow_links):
                                                if not profile_link:  # Only set if we haven't found a specific profile link
                                                    profile_link = full_url
                                                    container_data['_profile_link'] = full_url
                                        elif sub_type == 'attribute':
                                            attr_name = sub_element.get('attribute_name', 'value') if isinstance(sub_element, dict) else getattr(sub_element, 'attribute_name', 'value')
                                            attr_value = ''
                                            if hasattr(sub_elements[0], 'get_attribute'):
                                                attr_value = sub_elements[0].get_attribute(attr_name) or ''
                                            elif hasattr(sub_elements[0], 'attrib'):
                                                attr_value = sub_elements[0].attrib.get(attr_name, '')
                                            container_data[sub_label] = attr_value
                                        elif sub_type == 'html':
                                            container_data[sub_label] = str(sub_elements[0])
                                        else:
                                            # Default to text
                                            text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                            container_data[sub_label] = text_content.strip() if text_content else ''
                                        
                                        logger.debug(f"Extracted {sub_label}: {container_data[sub_label]}")
                                        progress.update(1, f"✓ Extracted {sub_label}")
                                    except Exception as extract_error:
                                        logger.warning(f"Error extracting value for {sub_label}: {extract_error}")
                                        progress.update(1, f"✗ Failed {sub_label}")
                                        # Don't add None values - skip failed extractions
                                else:
                                    if sub_required:
                                        logger.warning(f"Required sub-element not found: {sub_label} with selector {sub_selector} in container {i}")
                                    progress.update(1, f"✗ Missing {sub_label}")
                                    # Don't add None values - skip missing elements
                                    
                            except Exception as e:
                                sub_label = sub_element.get('label') if isinstance(sub_element, dict) else getattr(sub_element, 'label', 'unknown')
                                logger.warning(f"Error processing sub-element {sub_label} from container {i}: {e}")
                                progress.update(1, f"✗ Error {sub_label}")
                                # Don't add None values - skip failed extractions
                        
                        # Auto-extract profile link if needed and not already found
                        if needs_profile_link and not profile_link:
                            try:
                                # Try to find any link within the container that points to a lawyer page
                                lawyer_links = container.css("a[href*='/lawyer/']")
                                if not lawyer_links:
                                    lawyer_links = container.css("a")  # Fallback to any link
                                
                                for link in lawyer_links:
                                    href = ''
                                    if hasattr(link, 'get_attribute'):
                                        href = link.get_attribute('href') or ''
                                    elif hasattr(link, 'attrib'):
                                        href = link.attrib.get('href', '')
                                    
                                    if href and ('/lawyer/' in href or href.startswith('/')):
                                        full_url = self.current_page.urljoin(href) if href else ''
                                        if full_url and '/lawyer/' in full_url:
                                            profile_link = full_url
                                            container_data['_profile_link'] = full_url
                                            logger.debug(f"Auto-extracted profile link for container {i}: {full_url}")
                                            break
                            except Exception as link_error:
                                logger.debug(f"Could not auto-extract profile link for container {i}: {link_error}")
                    
                    # If no sub-elements defined, extract the container's text
                    if not container_data or all(v is None for v in container_data.values()):
                        try:
                            text_content = container.text if hasattr(container, 'text') else str(container)
                            container_data['text'] = text_content.strip() if text_content else ''
                        except Exception as text_error:
                            logger.warning(f"Error extracting text from container {i}: {text_error}")
                            container_data['text'] = ''
                    
                            # Handle subpage navigation if profile link exists and subpage elements are defined
                    if (profile_link and 
                        hasattr(element_config, 'subpage_elements') and 
                        element_config.subpage_elements):
                        
                        logger.info(f"Navigating to subpage for container {i}: {profile_link}")
                        subpage_data = self._extract_subpage_data(profile_link, element_config.subpage_elements)
                        
                        if subpage_data:
                            # Merge subpage data with container data
                            container_data.update(subpage_data)
                            logger.debug(f"Merged subpage data for container {i}: {list(subpage_data.keys())}")
                    
                    # Handle containers that should extract data from subpages (education, credentials)
                    elif (profile_link and 
                          self._is_subpage_container(element_config)):
                        
                        logger.info(f"Container '{element_config.label}' needs subpage data - navigating to: {profile_link}")
                        subpage_data = self._extract_subpage_container_data(profile_link, element_config)
                        
                        if subpage_data:
                            # Replace container data with subpage data
                            container_data.update(subpage_data)
                            logger.debug(f"Extracted subpage container data for {element_config.label}: {list(subpage_data.keys())}")
                    
                    # Handle legacy follow_links behavior (for single container with follow_links=True)
                    elif (hasattr(element_config, 'follow_links') and 
                          element_config.follow_links and 
                          profile_link and 
                          not hasattr(element_config, 'subpage_elements')):
                        
                        logger.info(f"Following link for container {i} (legacy mode): {profile_link}")
                        # This will be handled by the existing _process_container_subpages method
                        container_data['_follow_link'] = True
                    
                    # Add container index for reference
                    container_data['_container_index'] = i
                    containers.append(container_data)
                    
                    # If no sub-elements were processed, update progress for this container
                    if not hasattr(element_config, 'sub_elements') or not element_config.sub_elements:
                        progress.update(1, f"Completed person {i+1}")
                    
                except Exception as container_error:
                    logger.warning(f"Error processing container {i}: {container_error}")
                    # Add empty container to maintain index consistency
                    containers.append({'_container_index': i, '_error': str(container_error)})
                    
                    # Update progress for failed container
                    if not hasattr(element_config, 'sub_elements') or not element_config.sub_elements:
                        progress.update(1, f"Failed person {i+1}")
            
            progress.finish(f"Extracted data from {len(containers)} people")
            logger.info(f"Successfully extracted data from {len(containers)} containers")
            return containers
            
        except Exception as e:
            logger.error(f"Error extracting container data: {e}")
            return []
    
    def _generate_pattern_selectors(self, original_selector: str) -> List[str]:
        """
        Generate alternative selectors for AutoMatch to try.
        
        Args:
            original_selector: The original CSS selector
            
        Returns:
            List of alternative selectors to try
        """
        patterns = []
        
        # If it's a class selector, try variations
        if original_selector.startswith('.'):
            class_name = original_selector[1:]
            patterns.extend([
                f'[class*="{class_name}"]',
                f'div.{class_name}',
                f'*[class~="{class_name}"]',
                f'div[class*="{class_name}"]',
                # Try partial class matches
                f'[class*="{class_name.split("-")[0] if "-" in class_name else class_name[:5]}"]'
            ])
        
        # If it's an ID selector
        elif original_selector.startswith('#'):
            id_name = original_selector[1:]
            patterns.extend([
                f'[id*="{id_name}"]',
                f'div#{id_name}',
                f'*[id~="{id_name}"]'
            ])
        
        # Tag-based patterns
        elif original_selector.isalpha():
            patterns.extend([
                f'{original_selector}[class]',
                f'{original_selector}[id]',
                f'div {original_selector}',
                f'main {original_selector}'
            ])
        
        # Complex selector patterns
        else:
            # Try breaking down complex selectors
            parts = original_selector.split()
            if len(parts) > 1:
                patterns.extend([
                    parts[-1],  # Last part only
                    ' '.join(parts[-2:]) if len(parts) >= 2 else parts[-1],  # Last two parts
                    f'[class*="{parts[-1].replace(".", "").replace("#", "")}"]'
                ])
        
        return patterns
    
    def _generate_fallback_xpaths(self, label: str, element_type: str) -> List[str]:
        """
        Generate fallback XPath patterns for common lawyer profile elements.
        
        Args:
            label: The label of the element (e.g., 'name', 'email', 'position')
            element_type: The type of element ('text', 'link', etc.)
            
        Returns:
            List of XPath patterns to try
        """
        label_lower = label.lower()
        patterns = []
        
        if 'name' in label_lower:
            patterns.extend([
                ".//a[contains(@href, '/lawyer/')]",  # Profile link text (Gibson Dunn style)
                ".//strong",  # Any strong tag
                ".//h1 | .//h2 | .//h3 | .//h4 | .//h5 | .//h6",  # Any heading
                ".//*[contains(@class, 'name')]",
                ".//*[contains(@class, 'title')]/strong",
                ".//p/strong",
                ".//div/strong",
                ".//span[contains(@class, 'name')]",
                ".//div[1]//text()[normalize-space()]",  # First div text content
                ".//text()[normalize-space() and string-length(.) > 3][1]"  # First meaningful text
            ])
        
        elif 'email' in label_lower:
            patterns.extend([
                ".//a[contains(@href, 'mailto:')]",
                ".//*[contains(text(), '@')]",
                ".//*[contains(@class, 'email')]",
                ".//a[contains(@href, '@')]"
            ])
        
        elif 'phone' in label_lower or 'telephone' in label_lower:
            patterns.extend([
                ".//a[contains(@href, 'tel:')]",
                ".//*[contains(text(), '(') and contains(text(), ')')]",
                ".//*[contains(@class, 'phone')]",
                ".//*[contains(text(), '-') and string-length(text()) > 10]"
            ])
        
        elif 'position' in label_lower or 'title' in label_lower:
            patterns.extend([
                ".//*[contains(text(), 'Partner') or contains(text(), 'Associate') or contains(text(), 'Counsel')]",
                ".//span[contains(text(), 'Partner') or contains(text(), 'Associate')]", 
                ".//p[contains(text(), 'Partner') or contains(text(), 'Associate')]",
                ".//*[contains(@class, 'position')]",
                ".//*[contains(@class, 'title')]",
                ".//span",  # Any span
                ".//div[contains(@class, 'position') or contains(@class, 'title')]",
                ".//p[2]",
                ".//text()[contains(., 'Partner') or contains(., 'Associate') or contains(., 'Counsel')]"
            ])
        
        elif 'education' in label_lower:
            patterns.extend([
                ".//*[contains(text(), 'University') or contains(text(), 'College')]",
                ".//*[contains(text(), 'J.D.') or contains(text(), 'LL.M')]",
                ".//*[contains(text(), 'Bachelor') or contains(text(), 'Master')]",
                ".//*[contains(@class, 'education')]",
                ".//li[contains(text(), 'University')]"
            ])
        
        elif 'cred' in label_lower or 'admission' in label_lower or 'bar' in label_lower:
            patterns.extend([
                ".//*[contains(text(), 'Bar') or contains(text(), 'Admission')]",
                ".//*[contains(text(), 'Licensed') or contains(text(), 'Admitted')]",
                ".//*[contains(@class, 'admission')]",
                ".//li[contains(text(), 'Bar')]"
            ])
        
        # Generic patterns for any element type
        patterns.extend([
            f".//*[contains(@class, '{label_lower}')]",
            f".//text()[contains(., '{label_lower.title()}')]",
            ".//p[position() <= 3]",  # First 3 paragraphs
            ".//span[position() <= 3]",  # First 3 spans
            ".//div[position() <= 3]"  # First 3 divs
        ])
        
        return patterns
    
    def _validate_extracted_elements(self, elements: List, label: str, element_type: str) -> List:
        """
        Validate extracted elements to ensure they contain reasonable content.
        
        Args:
            elements: List of elements to validate
            label: The label of the element (e.g., 'name', 'position')
            element_type: The type of element ('text', 'link', etc.)
            
        Returns:
            List of validated elements or empty list if validation fails
        """
        if not elements:
            return []
        
        label_lower = label.lower()
        valid_elements = []
        
        for element in elements:
            try:
                if element_type == 'text':
                    text = element.text if hasattr(element, 'text') else str(element)
                    text = text.strip() if text else ''
                    
                    if not text or len(text) < 2:
                        continue
                    
                    # Validate based on label type
                    if 'name' in label_lower:
                        if self._is_valid_name(text):
                            valid_elements.append(element)
                    elif 'position' in label_lower or 'title' in label_lower:
                        if self._is_valid_position(text):
                            valid_elements.append(element)
                    elif 'email' in label_lower:
                        if '@' in text and '.' in text:
                            valid_elements.append(element)
                    else:
                        # For other types, just check it's not common junk
                        if not self._is_common_junk(text):
                            valid_elements.append(element)
                            
                elif element_type == 'link':
                    href = ''
                    if hasattr(element, 'get_attribute'):
                        href = element.get_attribute('href') or ''
                    elif hasattr(element, 'attrib'):
                        href = element.attrib.get('href', '')
                    
                    if href and (href.startswith('http') or href.startswith('mailto:') or href.startswith('tel:')):
                        valid_elements.append(element)
                else:
                    # For other types, include all elements
                    valid_elements.append(element)
                    
            except Exception as e:
                logger.debug(f"Error validating element: {e}")
                continue
        
        # Return elements only if we found reasonable matches
        if len(valid_elements) > 0 and len(valid_elements) <= len(elements) * 0.8:  # At most 80% of elements should be valid
            return valid_elements
        
        return []
    
    def _is_valid_name(self, text: str) -> bool:
        """Check if text looks like a person's name."""
        # Should have proper case and reasonable length
        if len(text) < 3 or len(text) > 60:
            return False
        
        # Should contain at least one space (first + last name)
        if ' ' not in text:
            return False
        
        # Should start with capital letter
        if not text[0].isupper():
            return False
        
        # Shouldn't contain common non-name words
        common_words = ['click', 'here', 'more', 'view', 'contact', 'email', 'phone', 'representative', 'experience']
        text_lower = text.lower()
        if any(word in text_lower for word in common_words):
            return False
        
        return True
    
    def _is_valid_position(self, text: str) -> bool:
        """Check if text looks like a job position."""
        # Should be reasonable length
        if len(text) < 3 or len(text) > 100:
            return False
        
        # Should contain common position words or not be junk
        position_words = ['partner', 'associate', 'counsel', 'director', 'manager', 'senior', 'junior', 'lead', 'head', 'chief']
        text_lower = text.lower()
        
        # Either contains position words or doesn't contain junk
        has_position_words = any(word in text_lower for word in position_words)
        has_junk = self._is_common_junk(text)
        
        return has_position_words or not has_junk
    
    def _is_common_junk(self, text: str) -> bool:
        """Check if text is common junk that shouldn't be extracted."""
        junk_phrases = [
            'your privacy choices', 'cookies', 'always active', 'global search', 
            'follow us', 'connect with us', 'scroll to top', 'click here', 'read more',
            'view more', 'load more', 'show more', 'copyright', 'all rights reserved'
        ]
        
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in junk_phrases)
    
    def _try_load_more_buttons(self) -> bool:
        """Try to find and click Load More buttons."""
        load_more_patterns = [
            'button:contains("Load More")',
            'button:contains("Show More")', 
            'button:contains("View More")',
            'a:contains("Load More")',
            'a:contains("Show More")',
            '.load-more',
            '.show-more',
            '.view-more',
            '[data-load-more]'
        ]
        
        for pattern in load_more_patterns:
            try:
                elements = self.current_page.css(pattern)
                if elements:
                    logger.info(f"Found Load More button with pattern: {pattern}")
                    # In Scrapling, we can't click, but we can note the pattern exists
                    return True
            except Exception:
                continue
        
        return False
    
    def _try_scrapling_scroll(self, scroll_page) -> bool:
        """Try scrolling using Scrapling's internal mechanisms."""
        try:
            # Try to access internal browser/page from Scrapling
            if hasattr(scroll_page, '_browser') and scroll_page._browser:
                # This might work with some versions of Scrapling
                scroll_page._browser.execute_script("window.scrollBy(0, 3000);")
                logger.debug("Scrolled using Scrapling browser")
                return True
            elif hasattr(self.fetcher, 'browser') and self.fetcher.browser:
                # Try fetcher's browser
                self.fetcher.browser.execute_script("window.scrollBy(0, 3000);")
                logger.debug("Scrolled using fetcher browser")
                return True
        except Exception as e:
            logger.debug(f"Scrapling scroll failed: {e}")
        
        return False
    
    def _try_pagination_load(self) -> bool:
        """Try to load more content via pagination."""
        pagination_patterns = [
            'a:contains("Next")',
            'a:contains(">")', 
            '.pagination a:last-child',
            '.next',
            '.page-next'
        ]
        
        for pattern in pagination_patterns:
            try:
                elements = self.current_page.css(pattern)
                if elements:
                    logger.info(f"Found pagination with pattern: {pattern}")
                    # Note: In Scrapling we can't navigate, but we detected the pattern
                    return True
            except Exception:
                continue
        
        return False
    
    def _make_selector_generic(self, selector: str) -> str:
        """
        Convert specific nth-child selectors to generic ones to find all similar elements.
        
        Args:
            selector: Original CSS selector
            
        Returns:
            Generic selector without nth-child restrictions
        """
        import re
        
        # Remove nth-child() selectors to make it generic
        generic_selector = re.sub(r':nth-child\(\d+\)', '', selector)
        
        # Remove > combinators at the end if they become orphaned
        generic_selector = re.sub(r'\s*>\s*$', '', generic_selector)
        
        # Clean up multiple spaces
        generic_selector = re.sub(r'\s+', ' ', generic_selector).strip()
        
        # If selector becomes too generic, try to keep some structure
        if len(generic_selector.split()) < 2 and '>' in selector:
            # Keep the last two parts
            parts = selector.split('>')
            if len(parts) >= 2:
                # Keep the last element but make it generic
                last_part = parts[-1].strip()
                second_last = parts[-2].strip()
                last_part_generic = re.sub(r':nth-child\(\d+\)', '', last_part).strip()
                generic_selector = f"{second_last} {last_part_generic}".strip()
        
        return generic_selector if generic_selector else selector
    
    def _extract_subpage_data(self, element_config: ElementSelector, main_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Navigate to sub-pages and extract additional data.
        
        Args:
            element_config: ElementSelector with subpage configuration
            main_data: Main page data containing profile links
            
        Returns:
            Enhanced data with subpage information
        """
        if not hasattr(element_config, 'follow_links') or not element_config.follow_links:
            return main_data
        
        if not hasattr(element_config, 'subpage_elements') or not element_config.subpage_elements:
            return main_data
        
        enhanced_data = main_data.copy()
        
        # Process each container item that has a profile link
        container_label = element_config.label
        container_data = enhanced_data.get(container_label, [])
        
        for item in container_data:
            if not isinstance(item, dict):
                continue
                
            # Look for profile link (check _profile_link first, then other fields)
            profile_link = item.get('_profile_link')
            if not profile_link:
                for key, value in item.items():
                    if ('link' in key.lower() or 'url' in key.lower() or 'href' in key.lower()) and isinstance(value, str) and value.startswith('http'):
                        profile_link = value
                        break
                        
            if not profile_link:
                continue
                
            try:
                logger.info(f"Navigating to subpage: {profile_link}")
                
                # Fetch the subpage
                subpage = self._fetch_page(profile_link)
                if not subpage:
                    logger.warning(f"Failed to fetch subpage: {profile_link}")
                    continue
                
                # Extract subpage data
                subpage_data = {}
                for sub_element in element_config.subpage_elements:
                    try:
                        if isinstance(sub_element, dict):
                            sub_label = sub_element.get('label')
                            sub_selector = sub_element.get('selector')
                            sub_type = sub_element.get('element_type', 'text')
                        else:
                            sub_label = sub_element.label
                            sub_selector = sub_element.selector
                            sub_type = sub_element.element_type
                        
                        # Use AutoMatch for subpage elements too
                        sub_elements = []
                        try:
                            sub_elements = subpage.css(sub_selector)
                        except Exception:
                            sub_elements = subpage.css(sub_selector)
                        
                        if sub_elements:
                            if sub_type == 'text':
                                text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                subpage_data[sub_label] = text_content.strip() if text_content else ''
                            elif sub_type == 'link':
                                href = ''
                                if hasattr(sub_elements[0], 'get_attribute'):
                                    href = sub_elements[0].get_attribute('href') or ''
                                elif hasattr(sub_elements[0], 'attrib'):
                                    href = sub_elements[0].attrib.get('href', '')
                                subpage_data[sub_label] = subpage.urljoin(href) if href else ''
                            elif sub_type == 'attribute':
                                attr_name = sub_element.get('attribute_name', 'value') if isinstance(sub_element, dict) else getattr(sub_element, 'attribute_name', 'value')
                                attr_value = ''
                                if hasattr(sub_elements[0], 'get_attribute'):
                                    attr_value = sub_elements[0].get_attribute(attr_name) or ''
                                elif hasattr(sub_elements[0], 'attrib'):
                                    attr_value = sub_elements[0].attrib.get(attr_name, '')
                                subpage_data[sub_label] = attr_value
                            elif sub_type == 'html':
                                subpage_data[sub_label] = str(sub_elements[0])
                            else:
                                text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                subpage_data[sub_label] = text_content.strip() if text_content else ''
                        else:
                            logger.debug(f"Subpage element not found: {sub_label} with selector {sub_selector}")
                            subpage_data[sub_label] = None
                            
                    except Exception as e:
                        logger.warning(f"Error extracting subpage element {sub_label}: {e}")
                        subpage_data[sub_label] = None
                
                # Merge subpage data into main item
                item.update(subpage_data)
                logger.info(f"Enhanced data for {item.get('name', 'Unknown')} with {len(subpage_data)} subpage fields")
                
                # Add small delay between subpage requests to be respectful
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"Error processing subpage {profile_link}: {e}")
                continue
        
        return enhanced_data
    
    def _process_container_subpages(self, scraped_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process containers with follow_links enabled for automatic subpage navigation.
        
        Args:
            scraped_data: Main page scraped data
            
        Returns:
            Enhanced data with subpage information from containers
        """
        enhanced_data = scraped_data.copy()
        
        # Check each element configuration for container subpage extraction
        for element in self.template.elements:
            if (hasattr(element, 'is_container') and element.is_container and 
                hasattr(element, 'follow_links') and element.follow_links):
                
                if hasattr(element, 'subpage_elements') and element.subpage_elements:
                    logger.info(f"Processing container subpage extraction for element: {element.label}")
                    
                    # Get the container data
                    container_data = enhanced_data.get(element.label, [])
                    if not isinstance(container_data, list):
                        continue
                    
                    # Process each container item for subpage data
                    for item in container_data:
                        if not isinstance(item, dict):
                            continue
                            
                        # Look for a profile link in the item (check _profile_link first, then other fields)
                        profile_link = item.get('_profile_link')
                        if not profile_link:
                            for key, value in item.items():
                                if ('link' in key.lower() or 'url' in key.lower() or 'href' in key.lower()) and isinstance(value, str) and value.startswith('http'):
                                    profile_link = value
                                    break
                        
                        if not profile_link:
                            continue
                            
                        try:
                            logger.info(f"Navigating to container subpage: {profile_link}")
                            
                            # Fetch the subpage
                            subpage = self._fetch_page(profile_link)
                            if not subpage:
                                logger.warning(f"Failed to fetch container subpage: {profile_link}")
                                continue
                            
                            # Extract subpage data
                            subpage_data = {}
                            for sub_element in element.subpage_elements:
                                try:
                                    if isinstance(sub_element, dict):
                                        sub_label = sub_element.get('label')
                                        sub_selector = sub_element.get('selector')
                                        sub_type = sub_element.get('element_type', 'text')
                                    else:
                                        sub_label = sub_element.label
                                        sub_selector = sub_element.selector
                                        sub_type = sub_element.element_type
                                    
                                    # Use AutoMatch for subpage elements too
                                    sub_elements = []
                                    try:
                                        sub_elements = subpage.css(sub_selector)
                                    except Exception:
                                        sub_elements = subpage.css(sub_selector)
                                    
                                    if sub_elements:
                                        if sub_type == 'text':
                                            text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                            subpage_data[sub_label] = text_content.strip() if text_content else ''
                                        elif sub_type == 'link':
                                            href = ''
                                            if hasattr(sub_elements[0], 'get_attribute'):
                                                href = sub_elements[0].get_attribute('href') or ''
                                            elif hasattr(sub_elements[0], 'attrib'):
                                                href = sub_elements[0].attrib.get('href', '')
                                            subpage_data[sub_label] = subpage.urljoin(href) if href else ''
                                        elif sub_type == 'attribute':
                                            attr_name = sub_element.get('attribute_name', 'value') if isinstance(sub_element, dict) else getattr(sub_element, 'attribute_name', 'value')
                                            attr_value = ''
                                            if hasattr(sub_elements[0], 'get_attribute'):
                                                attr_value = sub_elements[0].get_attribute(attr_name) or ''
                                            elif hasattr(sub_elements[0], 'attrib'):
                                                attr_value = sub_elements[0].attrib.get(attr_name, '')
                                            subpage_data[sub_label] = attr_value
                                        elif sub_type == 'html':
                                            subpage_data[sub_label] = str(sub_elements[0])
                                        else:
                                            text_content = sub_elements[0].text if hasattr(sub_elements[0], 'text') else str(sub_elements[0])
                                            subpage_data[sub_label] = text_content.strip() if text_content else ''
                                    else:
                                        logger.debug(f"Container subpage element not found: {sub_label} with selector {sub_selector}")
                                        subpage_data[sub_label] = None
                                        
                                except Exception as e:
                                    logger.warning(f"Error extracting container subpage element {sub_label}: {e}")
                                    subpage_data[sub_label] = None
                            
                            # Merge subpage data into main item
                            item.update(subpage_data)
                            logger.info(f"Enhanced container item with {len(subpage_data)} subpage fields")
                            
                            # Add small delay between subpage requests to be respectful
                            time.sleep(1)
                            
                        except Exception as e:
                            logger.error(f"Error processing container subpage {profile_link}: {e}")
                            continue
        
        return enhanced_data
    
    def _handle_action_subpage_navigation(self, action: NavigationAction, main_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle action-based subpage navigation for containers.
        
        Args:
            action: NavigationAction with subpage navigation enabled
            main_data: Main page data containing containers
            
        Returns:
            Enhanced data with subpage information from action navigation
        """
        enhanced_data = {}
        
        try:
            # Find containers in main data that might have profile links
            for element in self.template.elements:
                if (hasattr(element, 'is_container') and element.is_container and 
                    element.label in main_data):
                    
                    container_data = main_data.get(element.label, [])
                    if not isinstance(container_data, list):
                        continue
                    
                    enhanced_container_data = []
                    
                    # Process each container item for subpage navigation
                    for item in container_data:
                        if not isinstance(item, dict):
                            enhanced_container_data.append(item)
                            continue
                        
                        # Look for a profile link in the item using the action's target pattern
                        profile_link = None
                        for key, value in item.items():
                            if (isinstance(value, str) and value.startswith('http') and 
                                action.target_url in value):
                                profile_link = value
                                break
                        
                        # If no direct match, try to construct the URL pattern
                        if not profile_link and action.target_url:
                            # Try to find a pattern that matches the action URL
                            for key, value in item.items():
                                if ('link' in key.lower() or 'url' in key.lower() or 'href' in key.lower()):
                                    if isinstance(value, str) and value.startswith('http'):
                                        profile_link = value
                                        break
                        
                        if profile_link:
                            try:
                                logger.info(f"Navigating to action subpage: {profile_link}")
                                
                                # Fetch the subpage
                                subpage = self._fetch_page(profile_link)
                                if not subpage:
                                    logger.warning(f"Failed to fetch action subpage: {profile_link}")
                                    enhanced_container_data.append(item)
                                    continue
                                
                                # Extract additional data from subpage using same template elements
                                subpage_data = {}
                                
                                # Re-run template extraction on subpage
                                for sub_element in self.template.elements:
                                    if not (hasattr(sub_element, 'is_container') and sub_element.is_container):
                                        try:
                                            sub_value = self._extract_element_data(sub_element)
                                            if sub_value is not None:
                                                subpage_data[f"subpage_{sub_element.label}"] = sub_value
                                        except Exception as e:
                                            logger.warning(f"Error extracting subpage element {sub_element.label}: {e}")
                                
                                # Merge subpage data into main item
                                enhanced_item = item.copy()
                                enhanced_item.update(subpage_data)
                                enhanced_item['_subpage_url'] = profile_link
                                enhanced_container_data.append(enhanced_item)
                                
                                logger.info(f"Enhanced container item with {len(subpage_data)} subpage fields")
                                
                                # Add small delay between subpage requests to be respectful
                                time.sleep(1)
                                
                            except Exception as e:
                                logger.error(f"Error processing action subpage {profile_link}: {e}")
                                enhanced_container_data.append(item)
                        else:
                            enhanced_container_data.append(item)
                    
                    enhanced_data[element.label] = enhanced_container_data
            
            return enhanced_data
            
        except Exception as e:
            logger.error(f"Error in action subpage navigation: {e}")
            return {}
    
    def _merge_directory_with_subpage_data(self, scraped_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge directory container data with subpage container data into the expected format.
        Expected format: [{"name": "...", "position": "...", "subinfo": {"education": "...", "creds": "..."}}]
        
        Args:
            scraped_data: Data with separate main and subpage containers
            
        Returns:
            Merged data in the expected format
        """
        try:
            # Check if we have both main and subpage data to merge
            main_data = scraped_data.get('main', [])
            subpage_data = scraped_data.get('subpage', []) or scraped_data.get('sublink', []) or scraped_data.get('sublink_container', [])
            
            if not main_data:
                logger.info("No main directory data to merge")
                return scraped_data
            
            logger.info(f"Merging {len(main_data)} main entries with {len(subpage_data)} subpage entries")
            
            merged_results = []
            
            # Process each main entry
            for i, main_entry in enumerate(main_data):
                if not isinstance(main_entry, dict):
                    continue
                
                # Create base entry with main data
                merged_entry = {
                    'name': main_entry.get('name'),
                    'position': main_entry.get('position'),
                    'sector': main_entry.get('sector'),
                    'email': main_entry.get('email'),
                    'profile_link': main_entry.get('_profile_link')
                }
                
                # Find corresponding subpage data by container index
                container_index = main_entry.get('_container_index')
                corresponding_subpage = None
                
                if container_index is not None and subpage_data:
                    for subpage_entry in subpage_data:
                        if (isinstance(subpage_entry, dict) and 
                            subpage_entry.get('_container_index') == container_index):
                            corresponding_subpage = subpage_entry
                            break
                
                # Add subinfo section with education and credentials
                subinfo = {}
                if corresponding_subpage:
                    education = corresponding_subpage.get('education')
                    creds = corresponding_subpage.get('creds')
                    
                    if education:
                        subinfo['education'] = education
                    if creds:
                        subinfo['credentials'] = creds
                
                # Only add subinfo if it has data
                if subinfo:
                    merged_entry['subinfo'] = subinfo
                
                merged_results.append(merged_entry)
            
            # Return the merged data in a clean format
            result = {'lawyers': merged_results}
            
            # Preserve any actions_executed data
            if 'actions_executed' in scraped_data:
                result['actions_executed'] = scraped_data['actions_executed']
            
            logger.info(f"Successfully merged data into {len(merged_results)} unified lawyer entries")
            return result
            
        except Exception as e:
            logger.error(f"Error merging directory and subpage data: {e}")
            return scraped_data
    
    def _process_subpage_extractions(self, scraped_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process subpage extractions for elements that have follow_links enabled.
        
        Args:
            scraped_data: Main page scraped data
            
        Returns:
            Enhanced data with subpage information
        """
        enhanced_data = scraped_data.copy()
        
        # Check each element configuration for subpage extraction
        for element in self.template.elements:
            if hasattr(element, 'follow_links') and element.follow_links:
                if hasattr(element, 'subpage_elements') and element.subpage_elements:
                    logger.info(f"Processing subpage extraction for element: {element.label}")
                    enhanced_data = self._extract_subpage_data(element, enhanced_data)
        
        return enhanced_data
    
    def _extract_multiple_elements(self, elements, element_config: ElementSelector) -> List[Any]:
        """Extract data from multiple elements."""
        results = []
        
        for element in elements:
            try:
                value = self._extract_single_element(element, element_config)
                if value is not None:
                    results.append(value)
            except Exception as e:
                logger.warning(f"Error extracting from multiple element: {e}")
                continue
        
        return results
    
    def _execute_actions(self) -> Dict[str, Any]:
        """
        Execute navigation actions defined in the template.
        
        Returns:
            Dictionary containing action execution results
        """
        action_results = {}
        
        for action in self.template.actions:
            try:
                logger.info(f"Executing action: {action.label}")
                
                result = self._execute_single_action(action)
                action_results[action.label] = result
                
                # Wait after action if specified
                if action.wait_after > 0:
                    logger.debug(f"Waiting {action.wait_after} seconds after action")
                    time.sleep(action.wait_after)
                
            except Exception as e:
                error_msg = f"Failed to execute action {action.label}: {str(e)}"
                logger.error(error_msg)
                action_results[action.label] = {'success': False, 'error': error_msg}
        
        return action_results
    
    def _execute_single_action(self, action: NavigationAction) -> Dict[str, Any]:
        """
        Execute a single navigation action with actual browser interaction.
        
        Args:
            action: NavigationAction to execute
            
        Returns:
            Dictionary containing execution result
        """
        if not self.current_page:
            raise Exception("No page available for action execution")
        
        try:
            # Find the action element using AutoMatch for better reliability
            action_elements = []
            
            # If action selector is generic and we need profile links, enhance it
            if action.selector == 'a' and self._template_needs_subpage_data():
                logger.info("Generic action selector detected - enhancing for profile links")
                enhanced_selectors = [
                    "a[href*='/lawyer/']",
                    "a[href*='/attorney/']", 
                    ".people.loading a",
                    "a"  # Fallback to original
                ]
                
                for enhanced_selector in enhanced_selectors:
                    try:
                        action_elements = self.current_page.css(enhanced_selector)
                        if action_elements and len(action_elements) <= 10:  # Reasonable number
                            logger.info(f"Enhanced action selector found {len(action_elements)} elements: {enhanced_selector}")
                            break
                    except Exception:
                        continue
            else:
                try:
                    action_elements = self.current_page.css(action.selector)
                except Exception:
                    action_elements = self.current_page.css(action.selector)
            
            if not action_elements:
                raise Exception(f"Action element not found: {action.selector}")
            
            if len(action_elements) > 20:
                logger.warning(f"Action selector '{action.selector}' matched {len(action_elements)} elements - using first one")
                action_elements = action_elements[:1]
            
            action_element = action_elements[0]
            
            # Execute the action based on type
            if action.action_type == 'click':
                logger.info(f"Executing click on: {action.selector}")
                
                # Try to get the target URL before clicking
                target_url = None
                if hasattr(action_element, 'get_attribute'):
                    target_url = action_element.get_attribute('href')
                elif hasattr(action_element, 'attrib'):
                    target_url = action_element.attrib.get('href')
                
                # Use page_action for reliable clicking
                def click_element(page):
                    try:
                        element_locator = page.locator(action.selector)
                        if element_locator.count() == 0:
                            logger.warning(f"Action element not found: {action.selector}")
                            return page
                        
                        # Scroll to element if needed
                        element_locator.scroll_into_view_if_needed()
                        
                        # Click the element
                        element_locator.click()
                        logger.info(f"Successfully clicked: {action.selector}")
                        
                        # Wait for any navigation or content changes
                        if target_url:
                            # If it's a link, wait for navigation
                            page.wait_for_load_state('networkidle', timeout=10000)
                        else:
                            # Wait a bit for dynamic content
                            page.wait_for_timeout(action.wait_after * 1000)
                        
                        return page
                        
                    except Exception as page_action_error:
                        logger.warning(f"Page action click failed: {page_action_error}")
                        return page
                
                try:
                    # Use page_action for clicking
                    fetch_options = {
                        'headless': self.template.headless,
                        'network_idle': True,
                        'timeout': self.template.wait_timeout * 1000,
                        'page_action': click_element
                    }
                    
                    # Get current URL or use target URL if it's a link
                    current_url = self.current_page.url
                    
                    # Re-fetch the page with the click action
                    updated_page = self.fetcher.fetch(current_url, **fetch_options)
                    
                    if updated_page and updated_page.status == 200:
                        self.current_page = updated_page
                        result = {'success': True, 'action': 'click_page_action', 'selector': action.selector, 'navigated_to': target_url}
                    else:
                        # Fallback to manual navigation if it's a link
                        if target_url:
                            full_url = self.current_page.urljoin(target_url)
                            logger.info(f"Fallback: navigating to link target: {full_url}")
                            self.current_page = self._fetch_page(full_url)
                            result = {'success': True, 'action': 'fallback_navigate', 'selector': action.selector, 'navigated_to': full_url}
                        else:
                            raise Exception("Page action failed and no target URL for fallback")
                
                except Exception as click_error:
                    logger.warning(f"Click action failed: {click_error}")
                    # Fallback to manual navigation if it's a link
                    if target_url:
                        full_url = self.current_page.urljoin(target_url)
                        logger.info(f"Final fallback: navigating to link target: {full_url}")
                        self.current_page = self._fetch_page(full_url)
                        result = {'success': True, 'action': 'fallback_navigate', 'selector': action.selector, 'navigated_to': full_url}
                    else:
                        raise click_error
                
            elif action.action_type == 'scroll':
                logger.info(f"Executing scroll to: {action.selector}")
                try:
                    if hasattr(self.current_page, 'browser') and hasattr(self.current_page.browser, 'page'):
                        playwright_page = self.current_page.browser.page
                        playwright_page.locator(action.selector).scroll_into_view_if_needed()
                        result = {'success': True, 'action': 'scroll', 'selector': action.selector}
                    else:
                        result = {'success': True, 'action': 'scroll_simulated', 'selector': action.selector}
                except Exception:
                    result = {'success': True, 'action': 'scroll_simulated', 'selector': action.selector}
                
            elif action.action_type == 'hover':
                logger.info(f"Executing hover on: {action.selector}")
                try:
                    if hasattr(self.current_page, 'browser') and hasattr(self.current_page.browser, 'page'):
                        playwright_page = self.current_page.browser.page
                        playwright_page.hover(action.selector)
                        result = {'success': True, 'action': 'hover', 'selector': action.selector}
                    else:
                        result = {'success': True, 'action': 'hover_simulated', 'selector': action.selector}
                except Exception:
                    result = {'success': True, 'action': 'hover_simulated', 'selector': action.selector}
                
            elif action.action_type == 'wait':
                logger.info(f"Waiting {action.wait_after} seconds at: {action.selector}")
                time.sleep(action.wait_after)
                result = {'success': True, 'action': 'wait', 'duration': action.wait_after}
                
            else:
                raise Exception(f"Unsupported action type: {action.action_type}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error executing action: {e}")
            return {'success': False, 'error': str(e)}
    
    def _get_page_title(self) -> str:
        """Get the title of the current page."""
        try:
            if self.current_page:
                title_elements = self.current_page.css('title')
                if title_elements:
                    return title_elements[0].text.strip()
            return ""
        except:
            return ""
    
    def _cleanup(self) -> None:
        """Clean up resources after scraping."""
        try:
            # Close the browser instance if it exists
            if self.fetcher_instance:
                logger.info("Closing browser session...")
                
                # Close all tracked pages first
                for page in self.browser_pages:
                    if page and hasattr(page, 'close'):
                        try:
                            page.close()
                        except Exception as e:
                            logger.warning(f"Error closing page: {e}")
                
                # Try to close the browser contexts
                if hasattr(self.fetcher_instance, 'browser') and self.fetcher_instance.browser:
                    try:
                        # Close all contexts
                        if hasattr(self.fetcher_instance.browser, 'contexts'):
                            for context in self.fetcher_instance.browser.contexts:
                                try:
                                    context.close()
                                except Exception as e:
                                    logger.warning(f"Error closing context: {e}")
                        
                        # Close the browser
                        self.fetcher_instance.browser.close()
                    except Exception as e:
                        logger.warning(f"Error closing browser: {e}")
                
                # Try to close the playwright instance
                if hasattr(self.fetcher_instance, 'playwright') and self.fetcher_instance.playwright:
                    try:
                        self.fetcher_instance.playwright.stop()
                    except Exception as e:
                        logger.warning(f"Error stopping playwright: {e}")
                
                self.fetcher_instance = None
                logger.info("Browser session closed successfully")
            
            # Clear tracked pages
            self.browser_pages.clear()
            
            logger.info("Scraping session cleanup completed")
        except Exception as e:
            logger.warning(f"Cleanup error: {e}")
    
    def __del__(self):
        """Destructor to ensure cleanup is called."""
        try:
            self._cleanup()
        except:
            pass  # Ignore errors in destructor
    
    def _generate_fallback_xpaths(self, label: str, element_type: str) -> List[str]:
        """
        Generate smart XPath fallbacks based on label and element type.
        Enhanced for Gibson Dunn and legal directory pages.
        
        Args:
            label: The semantic label (e.g., 'name', 'email', 'phone')
            element_type: The expected element type ('text', 'link', etc.)
            
        Returns:
            List of XPath expressions to try as fallbacks
        """
        fallback_xpaths = []
        
        label_lower = label.lower()
        
        # Enhanced email detection for Gibson Dunn
        if any(keyword in label_lower for keyword in ['email', 'emaik', 'mail']):
            fallback_xpaths.extend([
                ".//a[contains(@href, 'mailto:')]",
                ".//*[contains(text(), '@gibsondunn.com')]",
                ".//*[contains(text(), '@') and contains(text(), '.com')]",
                ".//a[contains(text(), '@')]",
                ".//*[@class and contains(@class, 'email')]",
                ".//*[contains(text(), '@') and contains(text(), '.')]"
            ])
        # Enhanced phone detection
        elif 'phone' in label_lower:
            fallback_xpaths.extend([
                ".//a[contains(@href, 'tel:')]",
                ".//*[contains(text(), '+1') or contains(text(), '(') or contains(text(), ')')]",
                ".//*[contains(text(), '.') and contains(text(), '.') and string-length(text()) > 10 and string-length(text()) < 20]",
                ".//*[@class and contains(@class, 'phone')]"
            ])
        # Enhanced name detection for lawyer profiles
        elif 'name' in label_lower:
            fallback_xpaths.extend([
                ".//h1[not(contains(@class, 'site-title'))]",
                ".//h2[not(contains(@class, 'widget-title'))]",
                ".//strong[position()=1 and string-length(normalize-space(text())) > 5]",
                ".//*[@class and (contains(@class, 'name') or contains(@class, 'lawyer') or contains(@class, 'person'))]",
                ".//strong[string-length(normalize-space(text())) > 5 and string-length(normalize-space(text())) < 50]",
                ".//*[contains(@class, 'entry-title') or contains(@class, 'page-title')]//text()[normalize-space()]",
                ".//h1//text()[normalize-space() and string-length(.) > 3]"
            ])
        # Enhanced title/position detection for legal professionals
        elif any(keyword in label_lower for keyword in ['title', 'position', 'job']):
            fallback_xpaths.extend([
                ".//*[contains(text(), 'Partner') or contains(text(), 'Associate') or contains(text(), 'Counsel') or contains(text(), 'Senior')]",
                ".//*[@class and (contains(@class, 'title') or contains(@class, 'position') or contains(@class, 'role'))]",
                ".//span[contains(text(), 'Partner') or contains(text(), 'Associate') or contains(text(), 'Of Counsel')]",
                ".//*[contains(text(), 'Practice') and contains(text(), 'Chair')]",
                ".//p[contains(text(), 'Partner') or contains(text(), 'Associate')]"
            ])
        # Enhanced practice area/sector detection
        elif any(keyword in label_lower for keyword in ['practice', 'sector', 'area']):
            fallback_xpaths.extend([
                ".//*[contains(text(), 'Finance') or contains(text(), 'Corporate') or contains(text(), 'Litigation')]",
                ".//*[contains(text(), 'Law') or contains(text(), 'Practice') or contains(text(), 'Legal')]",
                ".//*[@class and (contains(@class, 'practice') or contains(@class, 'capability'))]",
                ".//a[contains(@href, '/practice/') or contains(@href, '/capabilities/')]",
                ".//*[contains(text(), 'M&A') or contains(text(), 'Securities') or contains(text(), 'Real Estate')]"
            ])
        # Enhanced education detection
        elif 'education' in label_lower:
            fallback_xpaths.extend([
                ".//*[contains(text(), 'University') or contains(text(), 'College') or contains(text(), 'School') or contains(text(), 'Law School')]",
                ".//*[contains(text(), 'J.D.') or contains(text(), 'LL.M.') or contains(text(), 'B.A.') or contains(text(), 'B.S.') or contains(text(), 'MBA')]",
                ".//li[contains(text(), 'University') or contains(text(), 'College')]",
                ".//*[contains(text(), 'Harvard') or contains(text(), 'Yale') or contains(text(), 'Stanford')]",
                ".//ul[@class='is-style-no-bullets']//li"
            ])
        # Enhanced credentials detection
        elif any(keyword in label_lower for keyword in ['cred', 'admission', 'bar']):
            fallback_xpaths.extend([
                ".//*[contains(text(), 'Bar') or contains(text(), 'Admission') or contains(text(), 'Admitted')]",
                ".//*[contains(text(), 'California') or contains(text(), 'New York') or contains(text(), 'State')]",
                ".//li[contains(text(), 'Bar') or contains(text(), 'Court')]",
                ".//ul[@class='is-style-no-bullets']//li[contains(text(), 'Admitted') or contains(text(), 'Bar')]"
            ])
        
        # Gibson Dunn specific structure fallbacks
        if not fallback_xpaths:
            # Try Gibson Dunn common patterns
            fallback_xpaths.extend([
                ".//*[@class and contains(@class, 'wp-block')]//text()[normalize-space()]",
                ".//div[contains(@class, 'entry-content')]//text()[normalize-space()]",
                ".//main//p//text()[normalize-space()]",
                ".//article//text()[normalize-space()]"
            ])
        
        # Generic fallbacks based on element type
        if element_type == 'link':
            fallback_xpaths.extend([
                ".//a[@href and @href != '#'][1]",
                ".//a[normalize-space(text()) and not(contains(@href, 'javascript'))]",
                ".//a[contains(@href, 'http') or starts-with(@href, '/')]"
            ])
        elif element_type == 'text':
            fallback_xpaths.extend([
                ".//*[text() and normalize-space(text()) and string-length(normalize-space(text())) > 2][1]",
                ".//text()[normalize-space() and string-length(normalize-space(.)) > 2][1]",
                ".//*[normalize-space(text())][1]"
            ])
        
        return fallback_xpaths
    
    def _extract_subpage_data(self, profile_url: str, subpage_elements: List) -> Dict[str, Any]:
        """
        Navigate to a profile subpage and extract additional data.
        
        Args:
            profile_url: URL of the profile page to visit
            subpage_elements: List of elements to extract from the subpage
            
        Returns:
            Dictionary containing extracted subpage data
        """
        subpage_data = {}
        original_url = self.current_page.url if self.current_page else None
        
        try:
            logger.debug(f"Navigating to subpage: {profile_url}")
            
            # Navigate to the profile page
            subpage = self._fetch_page(profile_url)
            if not subpage:
                logger.warning(f"Failed to fetch subpage: {profile_url}")
                return {}
            
            # Store original page reference
            original_page = self.current_page
            self.current_page = subpage
            
            # Extract data from subpage using the defined elements
            for sub_element in subpage_elements:
                try:
                    # Handle both dict and object sub-elements
                    if isinstance(sub_element, dict):
                        sub_label = sub_element.get('label')
                        sub_selector = sub_element.get('selector')
                        sub_type = sub_element.get('element_type', 'text')
                        sub_required = sub_element.get('is_required', False)
                        sub_multiple = sub_element.get('is_multiple', False)
                    else:
                        sub_label = sub_element.label
                        sub_selector = sub_element.selector
                        sub_type = sub_element.element_type
                        sub_required = getattr(sub_element, 'is_required', False)
                        sub_multiple = getattr(sub_element, 'is_multiple', False)
                    
                    if not sub_selector:
                        continue
                    
                    logger.debug(f"Extracting subpage element: {sub_label} with selector: {sub_selector}")
                    
                    # Find elements on the subpage
                    elements = []
                    try:
                        if sub_selector.startswith('xpath:'):
                            xpath_expr = sub_selector[6:]
                            elements = self.current_page.xpath(xpath_expr)
                        else:
                            elements = self.current_page.css(sub_selector)
                            if not elements:
                                elements = self.current_page.css(sub_selector)
                    except Exception as selector_error:
                        logger.warning(f"Subpage selector failed for {sub_label}: {selector_error}")
                        
                        # Try fallback XPaths for known patterns
                        if any(keyword in sub_label.lower() for keyword in ['education', 'experience', 'bio', 'practice']):
                            fallback_xpaths = self._generate_fallback_xpaths(sub_label, sub_type)
                            for fallback_xpath in fallback_xpaths:
                                try:
                                    elements = self.current_page.xpath(fallback_xpath)
                                    if elements:
                                        logger.info(f"Subpage fallback XPath worked for {sub_label}: {fallback_xpath}")
                                        break
                                except Exception:
                                    continue
                    
                    if elements:
                        if sub_multiple:
                            # Extract multiple elements
                            extracted_values = []
                            for element in elements[:10]:  # Limit to first 10 to avoid too much data
                                value = self._extract_element_value(element, sub_type)
                                if value and value.strip():
                                    extracted_values.append(value.strip())
                            subpage_data[sub_label] = extracted_values
                        else:
                            # Extract single element
                            value = self._extract_element_value(elements[0], sub_type)
                            subpage_data[sub_label] = value.strip() if value else ''
                        
                        logger.debug(f"Extracted {sub_label} from subpage: {str(subpage_data[sub_label])[:100]}...")
                    
                    elif sub_required:
                        logger.warning(f"Required subpage element not found: {sub_label}")
                        subpage_data[sub_label] = None
                    else:
                        subpage_data[sub_label] = None
                        
                except Exception as element_error:
                    logger.warning(f"Error extracting subpage element {sub_label}: {element_error}")
                    subpage_data[sub_label] = None
            
            logger.info(f"Successfully extracted {len(subpage_data)} elements from subpage")
            
        except Exception as e:
            logger.error(f"Error during subpage extraction: {e}")
        
        finally:
            # Restore original page
            if original_page:
                self.current_page = original_page
                logger.debug(f"Returned to main page: {original_url}")
        
        return subpage_data
    
    def _extract_element_value(self, element, element_type: str) -> str:
        """Extract value from an element based on its type."""
        try:
            if element_type == 'text':
                return element.text if hasattr(element, 'text') else str(element)
            elif element_type == 'link':
                href = ''
                if hasattr(element, 'get_attribute'):
                    href = element.get_attribute('href') or ''
                elif hasattr(element, 'attrib'):
                    href = element.attrib.get('href', '')
                return self.current_page.urljoin(href) if href else ''
            elif element_type == 'attribute':
                # For attribute type, we'd need the attribute name, defaulting to 'value'
                attr_name = 'value'
                if hasattr(element, 'get_attribute'):
                    return element.get_attribute(attr_name) or ''
                elif hasattr(element, 'attrib'):
                    return element.attrib.get(attr_name, '')
            else:
                # Default to text
                return element.text if hasattr(element, 'text') else str(element)
        except Exception as e:
            logger.warning(f"Error extracting element value: {e}")
            return ''
    
    def _handle_filters_and_pagination(self) -> Dict[str, Any]:
        """
        Handle filter iterations and pagination for comprehensive data extraction.
        
        Returns:
            Dictionary containing all extracted data across filters and pages
        """
        all_data = {}
        
        try:
            # Check if template has filters defined
            if hasattr(self.template, 'filters') and self.template.filters:
                logger.info(f"Processing {len(self.template.filters)} filter configurations")
                all_data = self._process_with_filters()
            else:
                # No filters, process with pagination only
                all_data = self._process_with_pagination()
            
            return all_data
            
        except Exception as e:
            logger.error(f"Error in filter/pagination handling: {e}")
            return {}
    
    def _process_with_filters(self) -> Dict[str, Any]:
        """
        Process scraping with filter iterations.
        
        Returns:
            Dictionary containing data from all filter combinations
        """
        all_filter_data = {}
        
        try:
            # Generate all filter combinations
            filter_combinations = self._generate_filter_combinations()
            
            for i, filter_combo in enumerate(filter_combinations):
                logger.info(f"Processing filter combination {i+1}/{len(filter_combinations)}: {filter_combo}")
                
                # Apply filters
                self._apply_filters(filter_combo)
                
                # Wait for content to load
                time.sleep(2)
                
                # Process pagination for this filter combination
                filter_data = self._process_with_pagination()
                
                # Store data with filter context
                filter_key = '_'.join([f"{f['label']}_{f['value']}" for f in filter_combo])
                all_filter_data[filter_key] = filter_data
                
                # Add delay between filter combinations
                time.sleep(1)
            
            return all_filter_data
            
        except Exception as e:
            logger.error(f"Error processing filters: {e}")
            return {}
    
    def _generate_filter_combinations(self) -> List[List[Dict[str, str]]]:
        """
        Generate all possible filter combinations.
        
        Returns:
            List of filter combinations
        """
        import itertools
        
        filter_options = []
        
        for filter_config in self.template.filters:
            if filter_config.filter_values:
                filter_options.append([
                    {'label': filter_config.filter_label, 'selector': filter_config.filter_selector, 'value': value}
                    for value in filter_config.filter_values
                ])
        
        # Generate cartesian product of all filter options
        if filter_options:
            return [list(combo) for combo in itertools.product(*filter_options)]
        else:
            return [[]]
    
    def _apply_filters(self, filter_combo: List[Dict[str, str]]) -> None:
        """
        Apply a combination of filters to the page.
        
        Args:
            filter_combo: List of filter configurations to apply
        """
        for filter_item in filter_combo:
            try:
                # This is a simplified implementation - in practice, you'd need
                # to interact with the actual browser through Playwright
                logger.info(f"Would apply filter: {filter_item['label']} = {filter_item['value']}")
                
                # For Scrapling, we would need to implement actual filter interaction
                # This might require switching to PlaywrightFetcher for interactive capabilities
                
            except Exception as e:
                logger.warning(f"Error applying filter {filter_item['label']}: {e}")
    
    def _process_with_pagination(self) -> Dict[str, Any]:
        """
        Process scraping with pagination support.
        
        Returns:
            Dictionary containing data from all pages
        """
        all_containers_data = {}
        pagination_attempts = []
        
        try:
            # Check if this is a directory template that might benefit from URL-based pagination
            if self._looks_like_directory_template():
                logger.info("Directory template detected - attempting automatic pagination detection")
                
                # Try URL-based pagination first (most reliable for directory pages)
                try:
                    pagination_data = self._try_url_based_pagination()
                    if pagination_data:
                        return pagination_data
                    pagination_attempts.append("URL-based pagination failed")
                except Exception as e:
                    pagination_attempts.append(f"URL-based pagination error: {str(e)}")
                    logger.debug(f"URL-based pagination failed: {e}")
                
                # Fall back to scroll/load more approaches only if URL failed
                try:
                    return self._try_scroll_based_pagination()
                except Exception as e:
                    pagination_attempts.append(f"Scroll-based pagination error: {str(e)}")
                    logger.debug(f"Scroll-based pagination failed: {e}")
            else:
                # For non-directory templates, use standard pagination
                try:
                    return self._try_standard_pagination()
                except Exception as e:
                    pagination_attempts.append(f"Standard pagination error: {str(e)}")
                    logger.debug(f"Standard pagination failed: {e}")
                
        except Exception as e:
            logger.error(f"Error in pagination processing: {e}")
        
        # Log all pagination attempts if they all failed
        if pagination_attempts:
            logger.info(f"All pagination methods failed: {'; '.join(pagination_attempts)}")
        
        # Return single page data as fallback
        return self._extract_data()
    
    def _handle_pagination(self) -> bool:
        """
        Handle pagination navigation.
        
        Returns:
            True if successfully navigated to next page, False if no more pages
        """
        try:
            pagination = self.template.pagination
            
            if pagination.pattern_type == 'button':
                # Look for next button
                next_buttons = self.current_page.css(pagination.next_selector) if pagination.next_selector else []
                if next_buttons and next_buttons[0].is_visible():
                    # For Scrapling, we can't actually click, but we can detect the pattern
                    logger.info("Next button detected - would navigate to next page")
                    return True
                
            elif pagination.pattern_type == 'infinite_scroll':
                # Handle infinite scroll
                return self._handle_infinite_scroll()
                
            elif pagination.pattern_type == 'load_more':
                return self._handle_load_more_pagination()
            
            return False
            
        except Exception as e:
            logger.warning(f"Error handling pagination: {e}")
            return False
    
    def _handle_infinite_scroll(self) -> bool:
        """
        Handle infinite scroll scenarios.
        
        Returns:
            True if more content available, False otherwise
        """
        try:
            # For Scrapling, we can't actually scroll, but we can implement
            # detection logic for content changes
            logger.info("Infinite scroll detected - would continue scrolling")
            
            # Check for end condition
            if hasattr(self.template.pagination, 'end_condition_selector') and self.template.pagination.end_condition_selector:
                end_elements = self.current_page.css(self.template.pagination.end_condition_selector)
                if end_elements:
                    logger.info("End condition met - stopping infinite scroll")
                    return False
            
            return True
            
        except Exception as e:
            logger.warning(f"Error in infinite scroll handling: {e}")
            return False
    
    def _handle_load_more_pagination(self) -> bool:
        """
        Handle load more button pagination using Scrapling's page_action.
        
        Returns:
            True if more content was loaded, False if no more content
        """
        try:
            pagination = self.template.pagination
            
            if not pagination.load_more_selector:
                return False
            
            # Ensure we're on the correct page (listing page, not individual profile page)
            if 'lawyer/' in self.current_page.url or '/people/' not in self.current_page.url:
                logger.warning(f"Not on listing page (current: {self.current_page.url}), navigating back to listing page")
                # Navigate back to the main listing page
                listing_page = self._fetch_page(self.template.url)
                if listing_page:
                    self.current_page = listing_page
                else:
                    logger.error("Failed to navigate back to listing page")
                    return False
            
            # Check if load more button exists on current page
            load_more_buttons = self.current_page.css(pagination.load_more_selector)
            if not load_more_buttons:
                logger.info("Load more button not found - end of content")
                return False
            
            logger.info(f"Clicking load more button: {pagination.load_more_selector}")
            
            # Define the page action to click load more button
            def click_load_more(page):
                try:
                    # Ensure we're on the right page
                    if 'lawyer/' in page.url:
                        logger.warning("Attempted to click load more on individual lawyer page - aborting")
                        return page
                    
                    # Check if button exists and is visible
                    button_locator = page.locator(pagination.load_more_selector)
                    if button_locator.count() == 0:
                        logger.info("Load more button not found during action")
                        return page
                    
                    if not button_locator.is_visible():
                        logger.info("Load more button not visible during action")
                        return page
                    
                    # Scroll to button first to ensure it's in viewport
                    button_locator.scroll_into_view_if_needed()
                    
                    # Click the button
                    button_locator.click()
                    logger.info("Load more button clicked successfully")
                    
                    # Wait for new content to load
                    wait_time = pagination.scroll_pause_time if pagination.scroll_pause_time else 3
                    page.wait_for_timeout(wait_time * 1000)  # Convert to milliseconds
                    
                    # Wait for network to be idle (new content loaded)
                    page.wait_for_load_state('networkidle', timeout=10000)
                    
                    return page
                    
                except Exception as action_error:
                    logger.warning(f"Error in page action: {action_error}")
                    return page
            
            # Fetch the listing page with the load more action
            try:
                fetch_options = {
                    'headless': self.template.headless,
                    'network_idle': True,
                    'timeout': self.template.wait_timeout * 1000,
                    'page_action': click_load_more
                }
                
                # Always use the main listing URL, not the current page URL
                listing_url = self.template.url
                updated_page = self.fetcher.fetch(listing_url, **fetch_options)
                
                if updated_page and updated_page.status == 200:
                    self.current_page = updated_page
                    logger.info("Page successfully updated after load more click")
                    return True
                else:
                    logger.warning("Failed to fetch updated page after load more click")
                    return False
                    
            except Exception as fetch_error:
                logger.warning(f"Error fetching page with load more action: {fetch_error}")
                return False
            
        except Exception as e:
            logger.warning(f"Error in load more handling: {e}")
            return False
    
    def _flatten_paginated_data(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Flatten paginated data into a single list.
        
        Args:
            pages_data: List of page data dictionaries
            
        Returns:
            Flattened list of all data items
        """
        flattened = []
        
        for page in pages_data:
            page_data = page.get('data', {})
            
            # Find container data (lists of items)
            for key, value in page_data.items():
                if isinstance(value, list) and value:
                    # If it's a list of dictionaries (container data), extend the flattened list
                    if isinstance(value[0], dict):
                        for item in value:
                            item['_source_page'] = page['page_number']
                            flattened.append(item)
                    else:
                        # Simple list values
                        for item in value:
                            flattened.append({
                                key: item,
                                '_source_page': page['page_number']
                            })
        
        return flattened
    
    def _try_url_based_pagination(self) -> Optional[Dict[str, Any]]:
        """
        Attempt URL-based pagination by detecting and testing common pagination parameters.
        
        Returns:
            Dictionary containing all paginated data, or None if pagination not possible
        """
        try:
            base_url = self.template.url
            parsed_url = urlparse(base_url)
            base_query = parse_qs(parsed_url.query)
            
            # Common pagination parameter patterns to test
            pagination_patterns = [
                {'param': 'offset', 'step': 20, 'start': 0},
                {'param': 'page', 'step': 1, 'start': 1},
                {'param': 'skip', 'step': 20, 'start': 0},
                {'param': 'start', 'step': 20, 'start': 0},
                {'param': 'from', 'step': 20, 'start': 0}
            ]
            
            # Test which pagination pattern works
            working_pattern = None
            for pattern in pagination_patterns:
                test_query = base_query.copy()
                test_query[pattern['param']] = [str(pattern['step'])]
                
                test_url = urlunparse((
                    parsed_url.scheme, parsed_url.netloc, parsed_url.path,
                    parsed_url.params, urlencode(test_query, doseq=True), parsed_url.fragment
                ))
                
                # Test if this pagination pattern works
                test_page = self._fetch_page(test_url)
                if test_page and self._has_different_content(test_page):
                    working_pattern = pattern
                    logger.info(f"Detected working pagination pattern: {pattern['param']}")
                    break
            
            if not working_pattern:
                logger.info("No URL-based pagination pattern detected")
                return None
            
            # Extract data from all pages using the working pattern
            all_data = {}
            current_offset = working_pattern['start']
            consecutive_empty_pages = 0
            max_pages = 200  # Safety limit
            page_count = 0
            
            while page_count < max_pages and consecutive_empty_pages < 3:
                # Build URL for current page
                page_query = base_query.copy()
                page_query[working_pattern['param']] = [str(current_offset)]
                
                page_url = urlunparse((
                    parsed_url.scheme, parsed_url.netloc, parsed_url.path,
                    parsed_url.params, urlencode(page_query, doseq=True), parsed_url.fragment
                ))
                
                logger.info(f"Fetching page {page_count + 1} with {working_pattern['param']}={current_offset}")
                
                # Fetch and extract data from this page
                page = self._fetch_page(page_url)
                if not page:
                    break
                
                self.current_page = page
                page_data = self._extract_data()
                
                if page_data:
                    # Check if page has new content
                    has_new_content = False
                    for key, value in page_data.items():
                        if isinstance(value, list) and value:
                            if key not in all_data:
                                all_data[key] = []
                            
                            # Check for duplicate content (indicating we've reached the end)
                            new_items = []
                            for item in value:
                                if item not in all_data[key]:
                                    new_items.append(item)
                                    has_new_content = True
                            
                            all_data[key].extend(new_items)
                        else:
                            if key not in all_data:
                                all_data[key] = value
                    
                    if not has_new_content:
                        consecutive_empty_pages += 1
                        logger.info(f"No new content found on page {page_count + 1}")
                    else:
                        consecutive_empty_pages = 0
                else:
                    consecutive_empty_pages += 1
                
                current_offset += working_pattern['step']
                page_count += 1
                
                # Add delay between requests
                time.sleep(1)
            
            if all_data:
                logger.info(f"URL-based pagination complete: extracted data from {page_count} pages")
                return all_data
            else:
                return None
                
        except Exception as e:
            logger.warning(f"Error in URL-based pagination: {e}")
            return None
    
    def _has_different_content(self, test_page) -> bool:
        """
        Check if a test page has different content than the current page.
        
        Args:
            test_page: Page to compare against current page
            
        Returns:
            True if pages have different content, False otherwise
        """
        try:
            if not self.current_page or not test_page:
                return False
            
            # Compare number of containers as a quick check
            current_containers = self._get_container_count(self.current_page)
            test_containers = self._get_container_count(test_page)
            
            # If container counts are different and both > 0, it's likely different content
            return test_containers > 0 and current_containers != test_containers
            
        except Exception:
            return False
    
    def _get_container_count(self, page) -> int:
        """
        Get the number of containers on a page for comparison.
        
        Args:
            page: Page to count containers on
            
        Returns:
            Number of containers found
        """
        try:
            # Try common directory container patterns
            selectors = [
                '.people-list strong',
                '[class*="people"] strong', 
                '.wpgb-grid > div',
                '.directory-item',
                '[class*="card"]',
                '[class*="item"]'
            ]
            
            for selector in selectors:
                try:
                    elements = page.css(selector)
                    if elements and len(elements) > 5:  # Must have substantial content
                        return len(elements)
                except Exception:
                    continue
            
            return 0
            
        except Exception:
            return 0
    
    def _try_scroll_based_pagination(self) -> Dict[str, Any]:
        """
        Fallback pagination using scroll/load more detection.
        
        Returns:
            Dictionary containing extracted data
        """
        try:
            logger.info("Attempting scroll-based pagination detection")
            
            # Try to auto-detect load more buttons
            detected_load_more = self._auto_detect_load_more_buttons()
            
            if detected_load_more:
                logger.info(f"Auto-detected load more button: {detected_load_more}")
                return self._try_auto_load_more_pagination(detected_load_more)
            
            # Try infinite scroll detection for WPGB
            if self._detect_wpgb_infinite_scroll():
                logger.info("WordPress Grid Builder infinite scroll detected")
                return self._try_wpgb_infinite_scroll_pagination()
            
            # First extract current page data
            current_data = self._extract_data()
            
            # Try load more approaches if available
            if hasattr(self.template, 'pagination') and self.template.pagination:
                return self._try_standard_pagination()
            
            return current_data
            
        except Exception as e:
            logger.warning(f"Error in scroll-based pagination: {e}")
            return self._extract_data()
    
    def _try_standard_pagination(self) -> Dict[str, Any]:
        """
        Standard pagination processing for configured templates.
        
        Returns:
            Dictionary containing all paginated data
        """
        all_containers_data = {}
        page_number = 1
        
        try:
            while True:
                logger.info(f"Processing page {page_number}")
                
                # Extract data from current page
                page_data = self._extract_data()
                
                if page_data:
                    # Aggregate container data across pages
                    for key, value in page_data.items():
                        if isinstance(value, list):
                            # This is likely container data
                            if key not in all_containers_data:
                                all_containers_data[key] = []
                            all_containers_data[key].extend(value)
                        else:
                            # Single value data - take from first page or latest
                            if key not in all_containers_data:
                                all_containers_data[key] = value
                
                # Handle pagination
                more_content_loaded = False
                if hasattr(self.template, 'pagination') and self.template.pagination:
                    if self.template.pagination.pattern_type == 'load_more':
                        more_content_loaded = self._handle_load_more_pagination()
                    else:
                        more_content_loaded = self._handle_pagination()
                    
                    if not more_content_loaded:
                        break
                else:
                    break
                
                page_number += 1
                
                # Safety check
                if hasattr(self.template.pagination, 'max_pages') and self.template.pagination.max_pages:
                    if page_number > self.template.pagination.max_pages:
                        break
                        
                # Add delay between requests
                if hasattr(self.template.pagination, 'scroll_pause_time'):
                    time.sleep(self.template.pagination.scroll_pause_time)
                else:
                    time.sleep(2)
        
            logger.info(f"Completed standard pagination processing. Total pages: {page_number}")
            return all_containers_data
            
        except Exception as e:
            logger.error(f"Error in standard pagination processing: {e}")
            return all_containers_data
    
    def _auto_detect_load_more_buttons(self) -> Optional[str]:
        """
        Automatically detect load more buttons on the current page.
        
        Returns:
            CSS selector for the load more button, or None if not found
        """
        try:
            # Common load more button patterns
            load_more_selectors = [
                # WordPress Grid Builder specific patterns
                '.wpgb-pagination-facet button',
                '.wpgb-pagination-facet a',
                '.wpgb-pagination-facet [class*="load"]',
                '.wpgb-pagination-facet [class*="more"]',
                '.wpgb-facet[class*="pagination"]',
                '.wpgb-load-more',
                '.wpgb-button[class*="load"]',
                '.wpgb-button[class*="more"]',
                # Generic pagination patterns
                'button[class*="load"]',
                'button[class*="more"]',
                'button[class*="show"]',
                'a[class*="load"]',
                'a[class*="more"]', 
                '[class*="load-more"]',
                '[class*="show-more"]',
                'button:contains("Load More")',
                'button:contains("Show More")',
                'button:contains("View More")',
                'a:contains("Load More")',
                'a:contains("Show More")',
                'a:contains("View More")',
                '.load-more',
                '.show-more',
                '.view-more',
                # Infinite scroll triggers
                '.infinite-scroll-trigger',
                '.infinite-scroll-button',
                '.lazy-load-trigger'
            ]
            
            for selector in load_more_selectors:
                try:
                    elements = self.current_page.css(selector)
                    if elements and len(elements) > 0:
                        # Check if the element is actually visible and clickable
                        element = elements[0]
                        if hasattr(element, 'is_visible') and element.is_visible():
                            logger.info(f"Found visible load more button with selector: {selector}")
                            return selector
                        elif hasattr(element, 'text') or hasattr(element, 'get_attribute'):
                            # Element exists, assume it's clickable
                            logger.info(f"Found load more button with selector: {selector}")
                            return selector
                except Exception:
                    continue
            
            logger.info("No load more buttons detected")
            return None
            
        except Exception as e:
            logger.warning(f"Error in load more button detection: {e}")
            return None
    
    def _try_auto_load_more_pagination(self, load_more_selector: str) -> Dict[str, Any]:
        """
        Attempt pagination using auto-detected load more button.
        
        Args:
            load_more_selector: CSS selector for the load more button
            
        Returns:
            Dictionary containing all paginated data
        """
        all_data = {}
        max_clicks = 200  # Safety limit
        clicks_performed = 0
        consecutive_failures = 0
        
        try:
            logger.info(f"Starting auto load more pagination with selector: {load_more_selector}")
            
            # Get initial data
            page_data = self._extract_data()
            if page_data:
                for key, value in page_data.items():
                    if isinstance(value, list):
                        all_data[key] = value.copy()
                    else:
                        all_data[key] = value
                        
                logger.info(f"Initial page loaded with {len(all_data.get('main_container', []))} items")
            
            while clicks_performed < max_clicks and consecutive_failures < 3:
                try:
                    # Check if load more button still exists
                    load_more_elements = self.current_page.css(load_more_selector)
                    if not load_more_elements:
                        logger.info("Load more button no longer found - pagination complete")
                        break
                    
                    # Use Scrapling's fetcher with page_action to click load more
                    def click_load_more_action(page):
                        try:
                            button_locator = page.locator(load_more_selector)
                            if button_locator.count() == 0:
                                logger.info("Load more button not found during click action")
                                return page
                            
                            if not button_locator.is_visible():
                                logger.info("Load more button not visible during click action")
                                return page
                            
                            # Scroll to button and click
                            button_locator.scroll_into_view_if_needed()
                            time.sleep(1)  # Small delay for scrolling
                            button_locator.click()
                            
                            logger.info(f"Clicked load more button (click #{clicks_performed + 1})")
                            
                            # Wait for new content to load
                            page.wait_for_timeout(3000)  # 3 second wait
                            page.wait_for_load_state('networkidle', timeout=10000)
                            
                            return page
                            
                        except Exception as action_error:
                            logger.warning(f"Error clicking load more button: {action_error}")
                            return page
                    
                    # Fetch updated page with load more action
                    fetch_options = {
                        'headless': self.template.headless,
                        'network_idle': True,
                        'timeout': self.template.wait_timeout * 1000,
                        'page_action': click_load_more_action
                    }
                    
                    updated_page = self.fetcher.fetch(self.template.url, **fetch_options)
                    
                    if updated_page and updated_page.status == 200:
                        self.current_page = updated_page
                        clicks_performed += 1
                        
                        # Extract new data
                        new_page_data = self._extract_data()
                        
                        if new_page_data:
                            # Check if we got new content
                            new_content_found = False
                            for key, value in new_page_data.items():
                                if isinstance(value, list) and value:
                                    if key not in all_data:
                                        all_data[key] = []
                                    
                                    # Count new items
                                    initial_count = len(all_data[key])
                                    
                                    # Add only new items (avoid duplicates)
                                    for item in value:
                                        if item not in all_data[key]:
                                            all_data[key].append(item)
                                            new_content_found = True
                                    
                                    new_count = len(all_data[key])
                                    logger.info(f"After click #{clicks_performed}: {key} has {new_count} items (added {new_count - initial_count})")
                                else:
                                    if key not in all_data:
                                        all_data[key] = value
                            
                            if new_content_found:
                                consecutive_failures = 0
                            else:
                                consecutive_failures += 1
                                logger.info(f"No new content found after click #{clicks_performed}")
                        else:
                            consecutive_failures += 1
                            logger.warning(f"Failed to extract data after click #{clicks_performed}")
                        
                        # Add delay between clicks
                        time.sleep(2)
                    else:
                        consecutive_failures += 1
                        logger.warning(f"Failed to fetch updated page after click #{clicks_performed}")
                        
                except Exception as click_error:
                    consecutive_failures += 1
                    logger.warning(f"Error during load more click #{clicks_performed}: {click_error}")
            
            total_items = len(all_data.get('main_container', []))
            logger.info(f"Auto load more pagination complete: performed {clicks_performed} clicks, collected {total_items} total items")
            
            return all_data
            
        except Exception as e:
            logger.error(f"Error in auto load more pagination: {e}")
            return all_data or self._extract_data()
    
    def _detect_wpgb_infinite_scroll(self) -> bool:
        """
        Detect if the page uses WordPress Grid Builder with infinite scroll.
        
        Returns:
            True if WPGB infinite scroll is detected, False otherwise
        """
        try:
            # Check for WPGB indicators
            wpgb_indicators = [
                '.wp-grid-builder',
                '.wpgb-template',
                '.wpgb-grid',
                '.wpgb-facet',
                '[data-grid]'
            ]
            
            for indicator in wpgb_indicators:
                elements = self.current_page.css(indicator)
                if elements:
                    logger.info(f"WPGB indicator found: {indicator}")
                    return True
            
            return False
            
        except Exception as e:
            logger.warning(f"Error detecting WPGB infinite scroll: {e}")
            return False
    
    def _try_wpgb_infinite_scroll_pagination(self) -> Dict[str, Any]:
        """
        Attempt pagination using WPGB infinite scroll by scrolling to trigger more content.
        
        Returns:
            Dictionary containing all paginated data
        """
        all_data = {}
        max_scrolls = 200  # Safety limit
        scrolls_performed = 0
        consecutive_no_new_content = 0
        
        try:
            logger.info("Starting WPGB infinite scroll pagination")
            
            # Get initial data
            page_data = self._extract_data()
            if page_data:
                for key, value in page_data.items():
                    if isinstance(value, list):
                        all_data[key] = value.copy()
                    else:
                        all_data[key] = value
                        
                initial_count = len(all_data.get('main_container', []))
                logger.info(f"Initial page loaded with {initial_count} items")
            
            while scrolls_performed < max_scrolls and consecutive_no_new_content < 5:
                try:
                    # Use Scrapling's fetcher with page_action to scroll and wait for new content
                    def scroll_and_wait_action(page):
                        try:
                            # Get current number of elements before scrolling
                            current_containers = page.query_selector_all('.people.loading')
                            before_count = len(current_containers) if current_containers else 0
                            
                            # Scroll to bottom to trigger infinite scroll
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            page.wait_for_timeout(2000)  # Wait for scroll to process
                            
                            # Wait for new content to potentially load
                            try:
                                # Wait for new elements to appear (or timeout after 5 seconds)
                                page.wait_for_function(
                                    f"document.querySelectorAll('.people.loading').length > {before_count}",
                                    timeout=5000
                                )
                                logger.info(f"New content loaded after scroll #{scrolls_performed + 1}")
                            except Exception:
                                # No new content appeared within timeout
                                logger.info(f"No new content after scroll #{scrolls_performed + 1}")
                            
                            # Additional wait for any lazy loading
                            page.wait_for_timeout(1000)
                            
                            return page
                            
                        except Exception as action_error:
                            logger.warning(f"Error during scroll action: {action_error}")
                            return page
                    
                    # Fetch updated page with scroll action
                    fetch_options = {
                        'headless': self.template.headless,
                        'network_idle': True,
                        'timeout': self.template.wait_timeout * 1000,
                        'page_action': scroll_and_wait_action
                    }
                    
                    updated_page = self.fetcher.fetch(self.template.url, **fetch_options)
                    
                    if updated_page and updated_page.status == 200:
                        self.current_page = updated_page
                        scrolls_performed += 1
                        
                        # Extract new data
                        new_page_data = self._extract_data()
                        
                        if new_page_data:
                            # Check if we got new content
                            new_content_found = False
                            for key, value in new_page_data.items():
                                if isinstance(value, list) and value:
                                    if key not in all_data:
                                        all_data[key] = []
                                    
                                    # Count new items
                                    initial_count = len(all_data[key])
                                    
                                    # Add only new items (avoid duplicates)
                                    for item in value:
                                        if item not in all_data[key]:
                                            all_data[key].append(item)
                                            new_content_found = True
                                    
                                    new_count = len(all_data[key])
                                    if new_count > initial_count:
                                        logger.info(f"After scroll #{scrolls_performed}: {key} has {new_count} items (added {new_count - initial_count})")
                                else:
                                    if key not in all_data:
                                        all_data[key] = value
                            
                            if new_content_found:
                                consecutive_no_new_content = 0
                            else:
                                consecutive_no_new_content += 1
                                logger.info(f"No new content found after scroll #{scrolls_performed}")
                        else:
                            consecutive_no_new_content += 1
                            logger.warning(f"Failed to extract data after scroll #{scrolls_performed}")
                        
                        # Add delay between scrolls
                        time.sleep(2)
                    else:
                        consecutive_no_new_content += 1
                        logger.warning(f"Failed to fetch updated page after scroll #{scrolls_performed}")
                        
                except Exception as scroll_error:
                    consecutive_no_new_content += 1
                    logger.warning(f"Error during scroll #{scrolls_performed}: {scroll_error}")
            
            total_items = len(all_data.get('main_container', []))
            logger.info(f"WPGB infinite scroll pagination complete: performed {scrolls_performed} scrolls, collected {total_items} total items")
            
            return all_data
            
        except Exception as e:
            logger.error(f"Error in WPGB infinite scroll pagination: {e}")
            return all_data or self._extract_data()

    def export_data(self, result: ScrapingResult, output_file: str, format: str = "json") -> None:
        """
        Export scraped data to various formats.
        
        Args:
            result: ScrapingResult containing the data to export
            output_file: Path to the output file
            format: Export format ('json', 'csv', 'excel')
        """
        try:
            # Ensure output directory exists
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            
            if format.lower() == 'json':
                self._export_json(result, output_file)
            elif format.lower() == 'csv':
                self._export_csv(result, output_file)
            elif format.lower() == 'excel':
                self._export_excel(result, output_file)
            else:
                raise ValueError(f"Unsupported export format: {format}")
            
            logger.info(f"Data exported successfully to: {output_file}")
            
        except Exception as e:
            logger.error(f"Export failed: {e}")
            raise
    
    def _export_json(self, result: ScrapingResult, output_file: str) -> None:
        """Export data to JSON format."""
        export_data = {
            'template_name': result.template_name,
            'url': result.url,
            'scraped_at': result.scraped_at.isoformat(),
            'success': result.success,
            'data': result.data,
            'metadata': result.metadata,
            'errors': result.errors
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
    
    def _export_csv(self, result: ScrapingResult, output_file: str) -> None:
        """Export data to CSV format."""
        # Flatten the data for CSV export
        flattened_data = self._flatten_data(result.data)
        
        if not flattened_data:
            raise ValueError("No data to export to CSV")
        
        # If data is a list of records, write multiple rows
        if isinstance(flattened_data, list):
            df = pd.DataFrame(flattened_data)
        else:
            # Single record
            df = pd.DataFrame([flattened_data])
        
        df.to_csv(output_file, index=False, encoding='utf-8')
    
    def _export_excel(self, result: ScrapingResult, output_file: str) -> None:
        """Export data to Excel format."""
        # Flatten the data for Excel export
        flattened_data = self._flatten_data(result.data)
        
        if not flattened_data:
            raise ValueError("No data to export to Excel")
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Main data sheet
            if isinstance(flattened_data, list):
                df = pd.DataFrame(flattened_data)
            else:
                df = pd.DataFrame([flattened_data])
            
            df.to_excel(writer, sheet_name='Scraped Data', index=False)
            
            # Metadata sheet
            metadata_df = pd.DataFrame([{
                'Template Name': result.template_name,
                'URL': result.url,
                'Scraped At': result.scraped_at.strftime('%Y-%m-%d %H:%M:%S'),
                'Success': result.success,
                'Elements Found': result.metadata.get('elements_found', 0),
                'Actions Executed': result.metadata.get('actions_executed', 0)
            }])
            
            metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
            
            # Errors sheet (if any)
            if result.errors:
                errors_df = pd.DataFrame([{'Error': error} for error in result.errors])
                errors_df.to_excel(writer, sheet_name='Errors', index=False)
    
    def _flatten_data(self, data: Dict[str, Any]) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
        """
        Flatten nested data structures for tabular export.
        
        Args:
            data: Dictionary containing scraped data
            
        Returns:
            Flattened data suitable for CSV/Excel export
        """
        flattened = {}
        
        for key, value in data.items():
            if key == 'actions_executed':
                continue  # Skip actions for tabular export
                
            if isinstance(value, list):
                if len(value) == 1:
                    flattened[key] = value[0]
                elif len(value) > 1:
                    # For multiple values, create separate columns
                    for i, item in enumerate(value):
                        flattened[f"{key}_{i+1}"] = item
                else:
                    flattened[key] = ""
            elif isinstance(value, dict):
                # Flatten nested dictionaries
                for subkey, subvalue in value.items():
                    flattened[f"{key}_{subkey}"] = subvalue
            else:
                flattened[key] = value
        
        return flattened


class BatchScraplingRunner:
    """
    Handles batch processing of multiple URLs using a single template.
    """
    
    def __init__(self, template: ScrapingTemplate):
        self.template = template
    
    def execute_batch(self, urls: List[str]) -> List[ScrapingResult]:
        """
        Execute scraping for multiple URLs using the same template.
        
        Args:
            urls: List of URLs to scrape
            
        Returns:
            List of ScrapingResult objects
        """
        results = []
        
        for i, url in enumerate(urls):
            try:
                logger.info(f"Processing URL {i+1}/{len(urls)}: {url}")
                
                # Create a copy of the template with the new URL
                url_template = self.template.model_copy()
                url_template.url = url
                
                # Execute scraping
                runner = ScraplingRunner(url_template)
                result = runner.execute_scraping()
                results.append(result)
                
                # Add delay between requests to be respectful
                if i < len(urls) - 1:  # Don't wait after the last URL
                    time.sleep(2)
                
            except Exception as e:
                logger.error(f"Failed to process URL {url}: {e}")
                
                # Create a failure result
                error_result = ScrapingResult(
                    template_name=self.template.name,
                    url=url,
                    success=False,
                    errors=[str(e)]
                )
                results.append(error_result)
        
        return results