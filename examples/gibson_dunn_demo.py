#!/usr/bin/env python3
"""
Demonstration of enhanced scraping features:
- AutoMatch integration for robust element selection
- Subpage navigation for comprehensive data extraction
- Action-based navigation for dynamic interactions

This demo shows how to extract Gibson Dunn lawyer data with full profiles.
"""

import json
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent))

from src.core.scrapling_runner_refactored import ScraplingRunner
from src.models.scraping_template import ScrapingTemplate

def demonstrate_enhanced_scraping():
    """Demonstrate the enhanced scraping capabilities"""
    
    print("ğŸ•·ï¸ Enhanced Web Scraping Demonstration")
    print("=" * 50)
    
    # Load the enhanced template
    template_path = Path(__file__).parent.parent / "templates" / "gibson_dunn_enhanced.json"
    
    if not template_path.exists():
        print(f"âŒ Template not found: {template_path}")
        return
    
    print(f"ğŸ“„ Loading template: {template_path.name}")
    
    # Load and parse template
    with open(template_path, 'r') as f:
        template_data = json.load(f)
    
    template = ScrapingTemplate(**template_data)
    
    print(f"âœ… Template loaded: {template.name}")
    print(f"ğŸ¯ Target URL: {template.url}")
    print(f"ğŸ“Š Elements configured: {len(template.elements)}")
    
    # Check for enhanced features
    enhanced_features = []
    for element in template.elements:
        if hasattr(element, 'follow_links') and element.follow_links:
            enhanced_features.append("Subpage Navigation")
        if hasattr(element, 'subpage_elements') and element.subpage_elements:
            enhanced_features.append("Enhanced Data Extraction")
    
    if template.actions:
        enhanced_features.append("Action-based Navigation")
    
    if template.pagination:
        enhanced_features.append("Pagination Support")
    
    print(f"ğŸš€ Enhanced features: {', '.join(enhanced_features)}")
    print()
    
    # Initialize and run scraper
    print("ğŸ¤– Starting enhanced scraping process...")
    runner = ScraplingRunner(template)
    
    try:
        result = runner.execute_scraping()
        
        if result.success:
            print("âœ… Scraping completed successfully!")
            print(f"ğŸ“ Data extracted for {len(result.data.get('lawyer_cards', []))} lawyers")
            
            # Show sample of enhanced data
            lawyers = result.data.get('lawyer_cards', [])
            if lawyers:
                sample_lawyer = lawyers[0]
                print(f"\nğŸ“‹ Sample lawyer data for: {sample_lawyer.get('name', 'Unknown')}")
                print(f"   ğŸ“§ Email: {sample_lawyer.get('email', 'N/A')}")
                print(f"   ğŸ¢ Title: {sample_lawyer.get('title', 'N/A')}")
                print(f"   ğŸ”— Profile: {sample_lawyer.get('profile_link', 'N/A')}")
                
                # Show subpage data if available
                subpage_fields = ['full_bio', 'education', 'experience', 'publications']
                subpage_data = {k: v for k, v in sample_lawyer.items() if k in subpage_fields and v}
                
                if subpage_data:
                    print(f"   ğŸ“š Enhanced data: {list(subpage_data.keys())}")
                else:
                    print(f"   ğŸ“š Enhanced data: None extracted (selectors may need adjustment)")
            
            # Show pagination results
            pages_data = result.data.get('pages', [])
            if pages_data:
                print(f"\nğŸ“„ Pagination results: {len(pages_data)} pages processed")
                total_lawyers = sum(len(page.get('data', {}).get('lawyer_cards', [])) for page in pages_data)
                print(f"   ğŸ‘¥ Total lawyers across all pages: {total_lawyers}")
        
        else:
            print("âŒ Scraping failed:")
            for error in result.errors:
                print(f"   â€¢ {error}")
    
    except Exception as e:
        print(f"âŒ Error during scraping: {e}")
    
    print("\nğŸ¯ Key Benefits of Enhanced Features:")
    print("   â€¢ AutoMatch: Adapts to website changes automatically")
    print("   â€¢ Subpage Navigation: Extracts comprehensive profile data")
    print("   â€¢ Action Navigation: Handles dynamic interactions")
    print("   â€¢ Intelligent Pagination: Scales to handle thousands of records")

if __name__ == "__main__":
    demonstrate_enhanced_scraping()